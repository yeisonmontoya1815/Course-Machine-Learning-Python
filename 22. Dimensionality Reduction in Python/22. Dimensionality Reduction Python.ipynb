{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **22. Dimensionality Reduction in Python**\n",
    "\n",
    "High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high dimensional data and you’ll be introduced to these in this course. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features. You’ll learn how to detect these features and drop them from the dataset so that you can focus on the informative ones. In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components.\n",
    "\n",
    "\n",
    "[reference Link](https://github.com/odenipinedo/Python/blob/master/datacamp/dimensionality%20reduction%20in%20Python.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 1 - Exploring High Dimensional Data**\n",
    "\n",
    "You'll be introduced to the concept of dimensionality reduction and will learn when an why this is important. You'll learn the difference between feature selection and feature extraction and will apply both techniques for data exploration. The chapter ends with a lesson on t-SNE, a powerful feature extraction technique that will allow you to visualize a high-dimensional dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Introduction**\n",
    "___\n",
    "- Tidy data\n",
    "    - every column is a feature\n",
    "    - every row is an observation for each variable\n",
    "    - Pandas dataframe .shape attribute\n",
    "- high dimensionality > 10 columns\n",
    "- When to use dimensionality reduction?\n",
    "    - drop columns with no variance (i.e. same values)\n",
    "    - Pandas dataframe .describe() method\n",
    "        - no variance = std = 0, max and min are the same\n",
    "        - exclude = 'number' --> will show information for non-numeric values\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the number of dimensions in a dataset\n",
    "A larger sample of the Pokemon dataset has been loaded for you as the pandas DataFrame **pokemon_df**. How many dimensions, or columns are in this dataset?\n",
    "\n",
    "```python\n",
    "In [2]:\n",
    "pokemon_df\n",
    "Out[2]:\n",
    "\n",
    "      HP  Attack  Defense  Generation                   Name     Type  Legendary\n",
    "0     45      49       49           1              Bulbasaur    Grass      False\n",
    "1     60      62       63           1                Ivysaur    Grass      False\n",
    "2     80      82       83           1               Venusaur    Grass      False\n",
    "3     80     100      123           1  VenusaurMega Venusaur    Grass      False\n",
    "4     39      52       43           1             Charmander     Fire      False\n",
    "..   ...     ...      ...         ...                    ...      ...        ...\n",
    "155  160     110       65           1                Snorlax   Normal      False\n",
    "156   41      64       45           1                Dratini   Dragon      False\n",
    "157   61      84       65           1              Dragonair   Dragon      False\n",
    "158   91     134       95           1              Dragonite   Dragon      False\n",
    "159  100     100      100           1                    Mew  Psychic      False\n",
    "\n",
    "[160 rows x 7 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Removing features without variance\n",
    "\n",
    "#A sample of the Pokemon dataset has been loaded as pokemon_df. To\n",
    "#get an idea of which features have little variance you should use\n",
    "#the IPython Shell to calculate summary statistics on this sample.\n",
    "#Then adjust the code to create a smaller, easier to understand,\n",
    "#dataset.\n",
    "\n",
    "# Leave this list as is\n",
    "number_cols = ['HP', 'Attack', 'Defense']\n",
    "\n",
    "# Remove the feature without variance from this list\n",
    "non_number_cols = ['Name', 'Type']\n",
    "\n",
    "# Create a new dataframe by subselecting the chosen features\n",
    "#df_selected = pokemon_df[number_cols + non_number_cols]\n",
    "\n",
    "# Prints the first 5 lines of the new dataframe\n",
    "#print(df_selected.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       HP  Attack  Defense                   Name   Type\n",
    "#    0  45      49       49              Bulbasaur  Grass\n",
    "#    1  60      62       63                Ivysaur  Grass\n",
    "#    2  80      82       83               Venusaur  Grass\n",
    "#    3  80     100      123  VenusaurMega Venusaur  Grass\n",
    "#    4  39      52       43             Charmander   Fire\n",
    "#################################################\n",
    "#All Pokemon in this dataset are non-legendary and from generation\n",
    "#one so you could choose to drop those two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Feature selection vs feature extraction**\n",
    "___\n",
    "- Why reduce dimensionality?\n",
    "    - your dataset will:\n",
    "        - be less complex\n",
    "        - require less disk space\n",
    "        - have lower chance of model overfitting\n",
    "- Feature selection\n",
    "    - .drop('column name', axis=1) [axis indicates column instead of row]\n",
    "- Building a pairplot\n",
    "    - sns.pairplot(data, hue='', diag_kind='hist')\n",
    "- Feature extraction\n",
    "    - calculating new feature(s) from original feature(s)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Visually detecting redundant features\n",
    "#Data visualization is a crucial step in any data exploration. Let's\n",
    "#use Seaborn to explore some samples of the US Army ANSUR body\n",
    "#measurement dataset.\n",
    "\n",
    "#Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2.\n",
    "\n",
    "#Seaborn has been imported as sns.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.1.svg](_images/13.1.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Two features are basically duplicates, remove one of them from\n",
    "#the dataset.\n",
    "\n",
    "# Remove one of the redundant features\n",
    "#reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.2.svg](_images/13.2.svg)\n",
    "the body height (inches) and stature (meters) hold the same information in a different unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now create a pairplot of the ansur_df_2 data sample and color the\n",
    "#points using the 'Gender' feature.\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.3.svg](_images/13.3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#One feature has no variance, remove it from the dataset.\n",
    "# Remove the redundant feature\n",
    "#reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "#sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.4.svg](_images/13.4.svg)\n",
    "all the individuals in the second sample have two legs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**t-SNE visualization of high-dimensional data**\n",
    "___\n",
    "- t-distributed stochastic neighbor embedding\n",
    "- t-SNE maximizes distance in 2-dimensional space between dimensions in higher dimensional space\n",
    "- does not work with non-numeric values\n",
    "- learning rate (10-1000) - lower number is conservative\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Fitting t-SNE to the ANSUR data\n",
    "\n",
    "#t-SNE is a great technique for visual exploration of high dimensional\n",
    "#datasets. In this exercise, you'll apply it to the ANSUR dataset. You'll\n",
    "#remove non-numeric columns from the pre-loaded dataset df and fit TSNE to\n",
    "#this numeric dataset.\n",
    "\n",
    "# Non-numerical columns in the dataset\n",
    "#non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "#df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Create a t-SNE model with learning rate 50\n",
    "#m = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "#tsne_features = m.fit_transform(df_numeric)\n",
    "\n",
    "#################################################\n",
    "#t-SNE reduced the more than 90 features in the dataset to just 2\n",
    "#which you can now plot.\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#t-SNE visualisation of dimensionality\n",
    "#Time to look at the results of your hard work. In this exercise,\n",
    "#you will visualize the output of t-SNE dimensionality reduction on\n",
    "#the combined male and female Ansur dataset. You'll create 3\n",
    "#scatterplots of the 2 t-SNE features ('x' and 'y') which were\n",
    "#added to the dataset df. In each scatterplot you'll color the\n",
    "#points according to a different categorical variable.\n",
    "\n",
    "#seaborn has already been imported as sns and matplotlib.pyplot as\n",
    "#plt.\n",
    "\n",
    "# Color the points according to Army Component\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Component', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.5.svg](_images/13.5.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Color the points by Army Branch\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Branch', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.6.svg](_images/13.6.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Color the points by Gender\n",
    "#sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.7.svg](_images/13.7.svg)\n",
    "There is a Male and a Female cluster. t-SNE found these gender\n",
    "differences in body shape without being told about them explicitly!\n",
    "\n",
    "From the second plot you learned there are more males in the\n",
    "Combat Arms Branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**The curse of dimensionality**\n",
    "___\n",
    "- as number of features increase in order to better fit a model, the number of observations must increase exponentially\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train - test split\n",
    "#In this chapter, you will keep working with the ANSUR dataset.\n",
    "#Before you can build a model on your dataset, you should first\n",
    "#decide on which feature you want to predict. In this case, you're\n",
    "#trying to predict gender.\n",
    "\n",
    "#You need to extract the column holding this feature from the\n",
    "#dataset and then split the data into a training and test set. The\n",
    "#training set will be used to train the model and the test set will\n",
    "#be used to check its performance on unseen data.\n",
    "\n",
    "#ansur_df has been pre-loaded for you.\n",
    "\n",
    "# Import train_test_split()\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "#y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "#X = ansur_df.drop('Gender', axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    300 rows in test set vs. 700 in training set. 91 Features.\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Fitting and testing the model\n",
    "\n",
    "#In the previous exercise, you split the dataset into X_train,\n",
    "#X_test, y_train, and y_test. These datasets have been pre-loaded\n",
    "#for you. You'll now create a support vector machine classifier\n",
    "#model (SVC()) and fit that to the training data. You'll then\n",
    "#calculate the accuracy on both the test and training set to detect\n",
    "#overfitting.\n",
    "\n",
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "#svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    49.7% accuracy on test set vs. 100.0% on training set\n",
    "#################################################\n",
    "#Looks like the model badly overfits on the training data. On unseen\n",
    "#data it performs worse than a random selector would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracy after dimensionality reduction\n",
    "\n",
    "#You'll reduce the overfit with the help of dimensionality reduction.\n",
    "#In this case, you'll apply a rather drastic form of dimensionality\n",
    "#reduction by only selecting a single column that has some good\n",
    "#information to distinguish between genders. You'll repeat the\n",
    "#train-test split, model fit and prediction steps to compare the\n",
    "#accuracy on test vs. training data.\n",
    "\n",
    "#All relevant packages and y have been pre-loaded.\n",
    "\n",
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "#X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#svc = SVC()\n",
    "#svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "#accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "#accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#   93.3% accuracy on test set vs. 94.9% on training set\n",
    "#################################################\n",
    "#On the full dataset the model is rubbish but with a single feature\n",
    "#we can make good predictions? This is an example of the curse of\n",
    "#dimensionality! The model badly overfits when we feed it too many\n",
    "#features. It overlooks that neck circumference by itself is pretty\n",
    "#different for males and females."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Features with missing values or little variance**\n",
    "___\n",
    "- Variance thresholds are not always easy to interpret or compare between features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Finding a good variance threshold\n",
    "\n",
    "#You'll be working on a slightly modified subsample of the ANSUR\n",
    "#dataset with just head measurements pre-loaded as head_df.\n",
    "\n",
    "#Create a boxplot on head_df.\n",
    "\n",
    "# Create the boxplot\n",
    "#head_df.boxplot()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.8.svg](_images/13.8.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Normalize the data by dividing the dataframe with its mean values.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "#normalized_df.boxplot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.9.svg](_images/13.9.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Print the variances of the normalized data.\n",
    "\n",
    "# Normalize the data\n",
    "#normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "#print(normalized_df.var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    headbreadth          1.678952e-03\n",
    "#    headcircumference    1.029623e-03\n",
    "#    headlength           1.867872e-03\n",
    "#    tragiontopofhead     2.639840e-03\n",
    "#    n_hairs              1.002552e-08\n",
    "#    measurement_error    3.231707e-27\n",
    "#    dtype: float64\n",
    "#################################################\n",
    "#Q: If you want to remove the 2 very low variance features.\n",
    "#What would be a good variance threshold?\n",
    "#A: 1.0e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Features with low variance\n",
    "\n",
    "#In the previous exercise you established that 0.001 is a good\n",
    "#threshold to filter out low variance features in head_df after\n",
    "#normalization. Now use the VarianceThreshold feature selector to\n",
    "#remove these features.\n",
    "\n",
    "#from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "#sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "#sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "#mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "#reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "#print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Dimensionality reduced from 6 to 4.\n",
    "#################################################\n",
    "#you've successfully removed the 2 low-variance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Removing features with many missing values\n",
    "\n",
    "#You'll apply feature selection on the Boston Public Schools\n",
    "#dataset which has been pre-loaded as school_df. Calculate the\n",
    "#missing value ratio per feature and then create a mask to remove\n",
    "#features with many missing values.\n",
    "\n",
    "#school_df.isna().sum() / len(school_df)\n",
    "#################################################\n",
    "#x             0.000000\n",
    "#y             0.000000\n",
    "#objectid_1    0.000000\n",
    "#objectid      0.000000\n",
    "#bldg_id       0.000000\n",
    "#bldg_name     0.000000\n",
    "#address       0.000000\n",
    "#city          0.000000\n",
    "#zipcode       0.000000\n",
    "#csp_sch_id    0.000000\n",
    "#sch_id        0.000000\n",
    "#sch_name      0.000000\n",
    "#sch_label     0.000000\n",
    "#sch_type      0.000000\n",
    "#shared        0.877863\n",
    "#complex       0.984733\n",
    "#label         0.000000\n",
    "#tlt           0.000000\n",
    "#pl            0.000000\n",
    "#point_x       0.000000\n",
    "#point_y       0.000000\n",
    "#dtype: float64\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create a boolean mask on whether each feature has less than 50%\n",
    "#missing values.\n",
    "\n",
    "#Apply the mask to school_df to select columns without many missing\n",
    "#values.\n",
    "\n",
    "# Create a boolean mask on whether each feature less than 50% missing values.\n",
    "#mask = school_df.isna().sum() / len(school_df) < 0.5\n",
    "\n",
    "# Create a reduced dataset by applying the mask\n",
    "#reduced_df = school_df.loc[:, mask]\n",
    "\n",
    "#print(school_df.shape)\n",
    "#print(reduced_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (131, 21)\n",
    "#    (131, 19)\n",
    "#################################################\n",
    "#The number of features went down from 21 to 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Pairwise correlation**\n",
    "___\n",
    "- pairplots\n",
    "- correlation coefficient\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Visualizing the correlation matrix\n",
    "\n",
    "#Reading the correlation matrix of ansur_df in its raw, numeric\n",
    "#format doesn't allow us to get a quick overview. Let's improve\n",
    "#this by removing redundant values and visualizing the matrix using\n",
    "#seaborn.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns, matplotlib.pyplot as plt,\n",
    "#NumPy as np and pandas as pd.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Draw the heatmap\n",
    "#sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.10.svg](_images/13.10.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create a boolean mask for the upper triangle of the plot.\n",
    "\n",
    "# Create the correlation matrix\n",
    "#corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "#mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "#sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.11.svg](_images/13.11.svg)\n",
    "The buttock and crotch height have a 0.93 correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Removing highly correlated features**\n",
    "___\n",
    "- correlation caveats - Anscombe's quartet\n",
    "    - nonlinear relationships or datasets with outliers may also correlate strongly\n",
    "    - always visualize the scatterplot\n",
    "- correlation does not imply causation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Filtering out highly correlated features\n",
    "#You're going to automate the removal of highly correlated features\n",
    "#in the numeric ANSUR dataset. You'll calculate the correlation\n",
    "#matrix and filter out columns that have a correlation coefficient\n",
    "#of more than 0.95 or less than -0.95.\n",
    "\n",
    "#Since each correlation coefficient occurs twice in the matrix\n",
    "#(correlation of A to B equals correlation of B to A) you'll want\n",
    "#to ignore half of the correlation matrix so that only one of the\n",
    "#two correlated features is removed. Use a mask trick for this\n",
    "#purpose.\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "#corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "#mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "#tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "#to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "#reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "#print(\"The reduced_df dataframe has {} columns\".format(reduced_df.shape[1]))\n",
    "\n",
    "#################################################\n",
    "#The original dataframe has 99 columns.\n",
    "#\n",
    "#<script.py> output:\n",
    "#    The reduced_df dataframe has 88 columns\n",
    "#################################################\n",
    "# You've automated the removal of highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Nuclear energy and pool drownings\n",
    "\n",
    "#The dataset that has been pre-loaded for you as weird_df contains\n",
    "#actual data provided by the US Centers for Disease Control &\n",
    "#Prevention and Department of Energy.\n",
    "\n",
    "#Let's see if we can find a pattern.\n",
    "\n",
    "#Seaborn has been pre-loaded as sns and matplotlib.pyplot as plt.\n",
    "\n",
    "# Print the first five lines of weird_df\n",
    "#print(weird_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       pool_drownings  nuclear_energy\n",
    "#    0             421           728.3\n",
    "#    1             465           753.9\n",
    "#    2             494           768.8\n",
    "#    3             538           780.1\n",
    "#    4             430           763.7\n",
    "#################################################\n",
    "\n",
    "#Create a scatterplot with nuclear energy production on the x-axis\n",
    "#and the number of pool drownings on the y-axis.\n",
    "\n",
    "#sns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.12.svg](_images/13.12.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the correlation matrix of weird_df\n",
    "#print(weird_df.corr())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    pool_drownings  nuclear_energy\n",
    "#    pool_drownings        1.000000        0.901179\n",
    "#    nuclear_energy        0.901179        1.000000\n",
    "#################################################\n",
    "#While the example is silly, you'll be amazed how often people\n",
    "#misunderstand correlation vs causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Selecting features for model performance**\n",
    "___\n",
    "- feature coefficients can be used to drop features that do not contribute to the model\n",
    "- Recursive Feature Elimination (RFE)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Building a diabetes classifier\n",
    "\n",
    "#You'll be using the Pima Indians diabetes dataset to predict whether\n",
    "#a person has diabetes using logistic regression. There are 8 features\n",
    "#and one target in this dataset. The data has been split into a\n",
    "#training and test set and pre-loaded for you as X_train, y_train,\n",
    "#X_test, and y_test.\n",
    "\n",
    "#A StandardScaler() instance has been predefined as scaler and a\n",
    "#LogisticRegression() one as lr.\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "#X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "#lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "#X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "#y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred)))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "#################################################\n",
    "#script.py> output:\n",
    "#    79.6% accuracy on test set.\n",
    "#    {'bmi': 0.38, 'glucose': 1.23, 'pregnant': 0.04, 'age': 0.34, 'triceps': 0.24, 'insulin': 0.19, 'family': 0.34, 'diastolic': 0.03}\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Manual Recursive Feature Elimination\n",
    "\n",
    "#Now that we've created a diabetes classifier, let's see if we can\n",
    "#reduce the number of features without hurting the model accuracy too\n",
    "#much.\n",
    "\n",
    "#On the second line of code the features are selected from the\n",
    "#original dataframe. Adjust this selection.\n",
    "\n",
    "#A StandardScaler() instance has been predefined as scaler and a\n",
    "#LogisticRegression() one as lr.\n",
    "\n",
    "#All necessary functions and packages have been pre-loaded too.\n",
    "\n",
    "#First, run the given code, then remove the feature with the lowest\n",
    "#model coefficient from X.\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "#X = diabetes_df[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#79.6% accuracy on test set.\n",
    "#{'bmi': 0.38, 'glucose': 1.23, 'pregnant': 0.04, 'age': 0.34, 'triceps': 0.24, 'insulin': 0.19, 'family': 0.34, 'diastolic': 0.03}\n",
    "#################################################\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "#X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    80.6% accuracy on test set.\n",
    "#    {'bmi': 0.39, 'glucose': 1.23, 'pregnant': 0.05, 'age': 0.35, 'triceps': 0.24, 'insulin': 0.2, 'family': 0.34}\n",
    "#################################################\n",
    "\n",
    "# Remove the 2 features with the lowest model coefficients\n",
    "#X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    79.6% accuracy on test set.\n",
    "#    {'triceps': 0.25, 'glucose': 1.13, 'family': 0.34, 'age': 0.37, 'bmi': 0.34}\n",
    "#################################################\n",
    "\n",
    "# Only keep the feature with the highest coefficient\n",
    "#X = diabetes_df[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "#lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "#acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "#print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    76.5% accuracy on test set.\n",
    "#    {'glucose': 1.27}\n",
    "#################################################\n",
    "#Removing all but one feature only reduced the accuracy by a few percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Automatic Recursive Feature Elimination\n",
    "\n",
    "#Now let's automate this recursive process. Wrap a Recursive Feature\n",
    "#Eliminator (RFE) around our logistic regression estimator and pass\n",
    "#it the desired number of features.\n",
    "\n",
    "#All the necessary functions and packages have been pre-loaded and\n",
    "#the features have been scaled for you.\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "#rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fit the eliminator to the data\n",
    "#rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "#print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "#print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "#acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 7 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 5 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    {'diastolic': 6, 'bmi': 1, 'glucose': 1, 'pregnant': 5, 'triceps': 3, 'age': 1, 'insulin': 4, 'family': 2}\n",
    "#    Index(['glucose', 'bmi', 'age'], dtype='object')\n",
    "#    80.6% accuracy on test set.\n",
    "#################################################\n",
    "#When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Tree-based feature selection**\n",
    "___\n",
    "- feature_importances_ attribute for features in random forest classifiers\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Building a random forest model\n",
    "\n",
    "#You'll again work on the Pima Indians dataset to predict whether\n",
    "#an individual has diabetes. This time using a random forest\n",
    "#classifier. You'll fit the model on the training data after\n",
    "#performing the train-test split and consult the feature importance\n",
    "#values.\n",
    "\n",
    "#The feature and target datasets have been pre-loaded for you as X\n",
    "#and y. Same goes for the necessary packages and functions.\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "#rf = RandomForestClassifier(random_state=0)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "#acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "#print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "#print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    {'diastolic': 0.08, 'bmi': 0.09, 'glucose': 0.21, 'pregnant': 0.09, 'triceps': 0.11, 'age': 0.16, 'insulin': 0.13, 'family': 0.12}\n",
    "#    77.6% accuracy on test set.\n",
    "#################################################\n",
    "#The random forest model gets 78% accuracy on the test set and 'glucose' is the most important feature (0.21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Random forest for feature selection\n",
    "#Now lets use the fitted random model to select the most important\n",
    "#features from our input dataset X.\n",
    "\n",
    "#The trained model from the previous exercise has been pre-loaded\n",
    "#for you as rf.\n",
    "\n",
    "#Create a mask for features with an importance higher than 0.15.\n",
    "# Create a mask for features importances above the threshold\n",
    "#mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Prints out the mask\n",
    "#print(mask)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [False  True False False False False False  True]\n",
    "#################################################\n",
    "\n",
    "#Sub-select the most important features by applying the mask to X.\n",
    "# Apply the mask to the feature dataset X\n",
    "#reduced_X = X.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['glucose', 'age'], dtype='object')\n",
    "#################################################\n",
    "#Only the features 'glucose' and 'age' were considered sufficiently important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination with random forests\n",
    "\n",
    "#You'll wrap a Recursive Feature Eliminator around a random forest\n",
    "#model to remove features step by step. This method is more\n",
    "#conservative compared to selecting features after applying a single\n",
    "#importance threshold. Since dropping one feature can influence the\n",
    "#relative importances of the others.\n",
    "\n",
    "#You'll need these pre-loaded datasets: X, X_train, y_train.\n",
    "\n",
    "#Functions and classes that have been pre-loaded for you are:\n",
    "#RandomForestClassifier(), RFE(), train_test_split().\n",
    "\n",
    "#Create a recursive feature eliminator that will select the 2 most\n",
    "#important features using a random forest model.\n",
    "\n",
    "# Wrap the feature eliminator around the random forest model\n",
    "#rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
    "\n",
    "#Fit the recursive feature eliminator to the training data.\n",
    "# Fit the model to the training data\n",
    "#rfe.fit(X_train, y_train)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 7 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 5 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    Fitting estimator with 3 features.\n",
    "#################################################\n",
    "\n",
    "#Create a mask using the fitted eliminator, then apply it to the\n",
    "#feature dataset X.\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "#mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "#reduced_X = X.loc[:, mask]\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['glucose', 'insulin'], dtype='object')\n",
    "#################################################\n",
    "\n",
    "#Change the settings of RFE() to eliminate 2 features at each step.\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "#rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "#rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "#mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "#reduced_X = X.loc[:, mask]\n",
    "#print(reduced_X.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 8 features.\n",
    "#    Fitting estimator with 6 features.\n",
    "#    Fitting estimator with 4 features.\n",
    "#    Index(['glucose', 'insulin'], dtype='object')\n",
    "#################################################\n",
    "#Compared to the quick and dirty single threshold method from the\n",
    "#previous exercise one of the selected features is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Regularized linear regression**\n",
    "___\n",
    "- .coef_ attribute\n",
    "- R squared value --> .score()\n",
    "- regularization --> simplify MSE values, solve for overfitting\n",
    "    - alpha - too low overfitting, too high innacurate / underfitting\n",
    "    - LASSO - least absolute shrinkage and selection operator\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a LASSO regressor\n",
    "\n",
    "#You'll be working on the numeric ANSUR body measurements dataset to\n",
    "#predict a persons Body Mass Index (BMI) using the pre-imported\n",
    "#Lasso() regressor. BMI is a metric derived from body height and\n",
    "#weight but those two features have been removed from the dataset\n",
    "#to give the model a challenge.\n",
    "\n",
    "#You'll standardize the data first using the StandardScaler() that\n",
    "#has been instantiated for you as scaler to make sure all\n",
    "#coefficients face a comparable regularizing force trying to bring\n",
    "#them down.\n",
    "\n",
    "#All necessary functions and classes plus the input datasets X and\n",
    "#y have been pre-loaded.\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "#X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "#la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "#la.fit(X_train_std, y_train)\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "#You've fitted the Lasso model to the standardized training data.\n",
    "#Now let's look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Lasso model results\n",
    "#Now that you've trained the Lasso model, you'll score its predictive\n",
    "#capacity (R2) on the test set and count how many features are\n",
    "#ignored because their coefficient is reduced to zero.\n",
    "\n",
    "#The X_test and y_test datasets have been pre-loaded for you.\n",
    "\n",
    "#The Lasso() model and StandardScaler() have been instantiated as\n",
    "#la and scaler respectively and both were fitted to the training data.\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "#X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "#r_squared = la.score(X_test_std, y_test)\n",
    "#print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "#zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "#n_ignored = sum(zero_coef)\n",
    "#print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The model can predict 84.7% of the variance in the test set.\n",
    "#    The model has ignored 82 out of 91 features.\n",
    "#################################################\n",
    "# We can predict almost 85% of the variance in the BMI value using\n",
    "#just 9 out of 91 of the features. The R^2 could be higher though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Adjusting the regularization strength\n",
    "#Your current Lasso model has an R2 score of 84.7%. When a model\n",
    "#applies overly powerful regularization it can suffer from high\n",
    "#bias, hurting its predictive power.\n",
    "\n",
    "#Let's improve the balance between predictive power and model\n",
    "#simplicity by tweaking the alpha parameter.\n",
    "\n",
    "#Find the highest value for alpha that gives an R2 value above 98%\n",
    "#from the options: 1, 0.5, 0.1, and 0.01.\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "#la = Lasso(alpha=0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "#la.fit(X_train_std, y_train)\n",
    "#r_squared = la.score(X_test_std, y_test)\n",
    "#n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats\n",
    "#print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "#print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n",
    "\n",
    "#################################################\n",
    "#alpha = 1\n",
    "#The model can predict 84.7% of the variance in the test set.\n",
    "#82 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.5\n",
    "#The model can predict 93.8% of the variance in the test set.\n",
    "#79 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.1\n",
    "#The model can predict 98.3% of the variance in the test set.\n",
    "#64 out of 91 features were ignored.\n",
    "\n",
    "#alpha = 0.01\n",
    "#The model can predict 98.8% of the variance in the test set.\n",
    "#37 out of 91 features were ignored.\n",
    "#################################################\n",
    "#With this more appropriate regularization strength we can predict\n",
    "#98% of the variance in the BMI value while ignoring 2/3 of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Combining feature selectors**\n",
    "___\n",
    "- LassoCV() automates the Lasso alpha selection process\n",
    "- Random forest is a combination of decision trees (weak predictors)\n",
    "- we can use combination of models for feature selection too (by rank votes)\n",
    "    - Gradient Boosting\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a LassoCV regressor\n",
    "\n",
    "#You'll be predicting biceps circumference on a subsample of the\n",
    "#male ANSUR dataset using the LassoCV() regressor that automatically\n",
    "#tunes the regularization strength (alpha value) using Cross-Validation.\n",
    "\n",
    "#The standardized training and test data has been pre-loaded for you\n",
    "#as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "#from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "#lcv = LassoCV()\n",
    "#lcv.fit(X_train, y_train)\n",
    "#print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "#r_squared = lcv.score(X_test, y_test)\n",
    "#print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "#lcv_mask = lcv.coef_ != 0\n",
    "#print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Optimal alpha = 0.089\n",
    "#    The model explains 88.2% of the test set variance\n",
    "#    26 features out of 32 selected\n",
    "#################################################\n",
    "#We got a decent R squared and removed 6 features. We'll save the\n",
    "#lcv_mask for later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Ensemble models for extra votes\n",
    "#The LassoCV() model selected 26 out of 32 features. Not bad, but\n",
    "#not a spectacular dimensionality reduction either. Let's use two\n",
    "#more models to select the 10 features they consider most important\n",
    "#using the Recursive Feature Eliminator (RFE).\n",
    "\n",
    "#The standardized training and test data has been pre-loaded for\n",
    "#you as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "#rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
    "#             n_features_to_select=10, step=3, verbose=1)\n",
    "#rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "#r_squared = rfe_gb.score(X_test, y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "#gb_mask = rfe_gb.support_\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 32 features.\n",
    "#    Fitting estimator with 29 features.\n",
    "#    Fitting estimator with 26 features.\n",
    "#    Fitting estimator with 23 features.\n",
    "#    Fitting estimator with 20 features.\n",
    "#    Fitting estimator with 17 features.\n",
    "#    Fitting estimator with 14 features.\n",
    "#    Fitting estimator with 11 features.\n",
    "#    The model can explain 85.6% of the variance in the test set\n",
    "#################################################\n",
    "\n",
    "#Modify the first step to select 10 features with RFE on a\n",
    "#RandomForestRegressor() and drop 3 features on each step.\n",
    "\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "#rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "#             n_features_to_select=10, step=3, verbose=1)\n",
    "#rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "#r_squared = rfe_rf.score(X_test, y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "#rf_mask = rfe_rf.support_\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Fitting estimator with 32 features.\n",
    "#    Fitting estimator with 29 features.\n",
    "#    Fitting estimator with 26 features.\n",
    "#    Fitting estimator with 23 features.\n",
    "#    Fitting estimator with 20 features.\n",
    "#    Fitting estimator with 17 features.\n",
    "#    Fitting estimator with 14 features.\n",
    "#    Fitting estimator with 11 features.\n",
    "#    The model can explain 84.0% of the variance in the test set\n",
    "#################################################\n",
    "#Inluding the Lasso linear model from the previous exercise, we\n",
    "#now have the votes from 3 models on which features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Combining 3 feature selectors\n",
    "#We'll combine the votes of the 3 models you built in the previous\n",
    "#exercises, to decide which features are important into a meta mask.\n",
    "#We'll then use this mask to reduce dimensionality and see how a\n",
    "#simple linear regressor performs on the reduced dataset.\n",
    "\n",
    "#The per model votes have been pre-loaded as lcv_mask, rf_mask, and\n",
    "#gb_mask and the feature and target datasets as X and y.\n",
    "\n",
    "# Sum the votes of the three models\n",
    "#votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "#print(votes)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [1 0 2 2 0 1 0 3 1 1 1 3 1 1 1 3 0 1 1 2 1 1 2 1 1 3 2 1 3 1 3 3]\n",
    "#################################################\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "#meta_mask = votes >= 3\n",
    "#print(meta_mask)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [False False False False False False False  True False False False  True\n",
    "#     False False False  True False False False False False False False False\n",
    "#     False  True False False  True False  True  True]\n",
    "#################################################\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "#X_reduced = X.loc[:,meta_mask]\n",
    "#print(X_reduced.columns)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['chestcircumference', 'forearmcircumferenceflexed', 'hipbreadth', 'thighcircumference', 'waistcircumference', 'wristheight', 'BMI'], dtype='object')\n",
    "#################################################\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "#lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "#r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "#print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The model can explain 86.8% of the variance in the test set using 7 features.\n",
    "#################################################\n",
    "#Using the votes from 3 models you were able to select just 7 features\n",
    "#that allowed a simple linear model to get a high accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Feature Extraction**\n",
    "___\n",
    "- calculate new features using existing ones while losing as little information as possible\n",
    "    - e.g., taking averages of multiple features\n",
    "- Principal Component Analysis (PCA)\n",
    "    - be sure to scale features\n",
    "    - multiply and sum components on perpendicular axes\n",
    "    - accounts for all variance in the data\n",
    "    - creates a new reference\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Manual feature extraction I\n",
    "\n",
    "#You want to compare prices for specific products between stores.\n",
    "#The features in the pre-loaded dataset sales_df are: storeID,\n",
    "#product, quantity and revenue. The quantity and revenue features\n",
    "#tell you how many items of a particular product were sold in a\n",
    "#store and what the total revenue was. For the purpose of your\n",
    "#analysis it's more interesting to know the average price per\n",
    "#product.\n",
    "\n",
    "#################################################\n",
    "#  storeID  product  quantity  revenue\n",
    "#0       A   Apples      1811   9300.6\n",
    "#1       A  Bananas      1003   3375.2\n",
    "#2       A  Oranges      1604   8528.5\n",
    "#3       B   Apples      1785   9181.0\n",
    "#4       B  Bananas       944   3680.2\n",
    "#################################################\n",
    "# Calculate the price from the quantity sold and revenue\n",
    "#sales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n",
    "\n",
    "# Drop the quantity and revenue features\n",
    "#reduced_df = sales_df.drop(['revenue', 'quantity'], axis=1)\n",
    "\n",
    "#print(reduced_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      storeID  product     price\n",
    "#    0       A   Apples  5.135616\n",
    "#    1       A  Bananas  3.365105\n",
    "#    2       A  Oranges  5.317020\n",
    "#    3       B   Apples  5.143417\n",
    "#    4       B  Bananas  3.898517\n",
    "#################################################\n",
    "#When you understand the dataset well, always check if you can\n",
    "#calculate relevant features and drop irrelevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Manual feature extraction II\n",
    "#You're working on a variant of the ANSUR dataset, height_df,\n",
    "#where a person's height was measured 3 times. Add a feature with\n",
    "#the mean height to the dataset, then drop the 3 original features.\n",
    "\n",
    "#################################################\n",
    "#  weight_kg  height_1  height_2  height_3\n",
    "#0       81.5      1.78      1.80      1.80\n",
    "#1       72.6      1.70      1.70      1.69\n",
    "#2       92.9      1.74      1.75      1.73\n",
    "#3       79.4      1.66      1.68      1.67\n",
    "#4       94.6      1.91      1.93      1.90\n",
    "#################################################\n",
    "# Calculate the mean height\n",
    "#height_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "#reduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)\n",
    "\n",
    "#print(reduced_df.head())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       weight_kg    height\n",
    "#    0       81.5  1.793333\n",
    "#    1       72.6  1.696667\n",
    "#    2       92.9  1.740000\n",
    "#    3       79.4  1.670000\n",
    "#    4       94.6  1.913333\n",
    "#################################################\n",
    "#You've calculated a new feature that is still easy to understand\n",
    "#compared to, for instance, principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Principal component analysis**\n",
    "___\n",
    "- PCA removes correlation\n",
    "- amount of features - amount of principal components\n",
    "- explained variance ratio - .explained_variance_ratio_\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Calculating Principal Components\n",
    "\n",
    "#You'll visually inspect a 4 feature sample of the ANSUR dataset\n",
    "#before and after PCA using Seaborn's pairplot(). This will allow\n",
    "#you to inspect the pairwise correlations between the features.\n",
    "\n",
    "#The data has been pre-loaded for you as ansur_df.\n",
    "\n",
    "# Create a pairplot to inspect ansur_df\n",
    "#sns.pairplot(ansur_df)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.13.svg](_images/13.13.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "#scaler = StandardScaler()\n",
    "#ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "#pca = PCA()\n",
    "#pc = pca.fit_transform(ansur_std)\n",
    "#pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "#sns.pairplot(pc_df)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.14.svg](_images/13.14.svg)\n",
    "Notice how, in contrast to the input features, none of the principal\n",
    "components are correlated to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA on a larger dataset\n",
    "\n",
    "#You'll now apply PCA on a somewhat larger ANSUR datasample with\n",
    "#13 dimensions, once again pre-loaded as ansur_df. The fitted model\n",
    "#will be used in the next exercise. Since we are not using the\n",
    "#principal components themselves there is no need to transform the\n",
    "#data, instead, it is sufficient to fit pca to the data.\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "#scaler = StandardScaler()\n",
    "#ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "#pca = PCA()\n",
    "#pca.fit(ansur_std)\n",
    "\n",
    "#You've fitted PCA on our 13 feature datasample. Now let's see how\n",
    "#the components explain the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA explained variance\n",
    "\n",
    "#You'll be inspecting the variance explained by the different\n",
    "#principal components of the pca instance you created in the\n",
    "#previous exercise.\n",
    "\n",
    "# Inspect the explained variance ratio per component\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759\n",
    "#     0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617\n",
    "#     0.00065268]\n",
    "#################################################\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.61449404 0.81343368 0.88146463 0.91916962 0.94948464 0.96666054\n",
    "#     0.97738816 0.98395496 0.99030239 0.99466254 0.99732115 0.99934732\n",
    "#     1.        ]\n",
    "#################################################\n",
    "#Using just 4 principal components we can explain more than 90%\n",
    "#of the variance in the 13 feature dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**PCA applications**\n",
    "___\n",
    "- remaining components can be hard to interpret\n",
    "- look at components attribute pca.components_\n",
    "    - to what extent does a components vector is affected by a specific feature\n",
    "- checking effect of categorical features\n",
    "    - hue category for scatterplot\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Understanding the components\n",
    "\n",
    "#You'll apply PCA to the numeric features of the Pokemon dataset,\n",
    "#poke_df, using a pipeline to combine the feature scaling and PCA\n",
    "#in one go. You'll then interpret the meanings of the first two\n",
    "#components.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "#pipe.fit(poke_df)\n",
    "#vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "#print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "#print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    PC 1 effects = {'Defense': 0.36, 'Attack': 0.44, 'Sp. Def': 0.45, 'Sp. Atk': 0.46, 'HP': 0.39, 'Speed': 0.34}\n",
    "#    PC 2 effects = {'Defense': 0.63, 'Attack': -0.01, 'Sp. Def': 0.24, 'Sp. Atk': -0.31, 'HP': 0.08, 'Speed': -0.67}\n",
    "#################################################\n",
    "#PC1: All features have a similar positive effect. PC 1 can be\n",
    "#interpreted as a measure of overall quality (high stats)\n",
    "#PC2: Defense has a strong positive effect on the second component\n",
    "#and speed a strong negative one. This component quantifies an\n",
    "#agility vs. armor & protection trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA for feature exploration\n",
    "\n",
    "#You'll use the PCA pipeline you've built in the previous exercise\n",
    "#to visually explore how some categorical features relate to the\n",
    "#variance in poke_df. These categorical features (Type & Legendary)\n",
    "#can be found in a separate dataframe poke_cat_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA())\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "#pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "#print(pc)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[-1.5563747  -0.02148212]\n",
    "#     [-0.36286656 -0.05026854]\n",
    "#     [ 1.28015158 -0.06272022]\n",
    "#     ...\n",
    "#     [ 2.45821626 -0.51588158]\n",
    "#     [ 3.5303971  -0.95106516]\n",
    "#     [ 2.23378629  0.53762985]]\n",
    "#################################################\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "#poke_cat_df['PC 1'] = pc[:, 0]\n",
    "#poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "#pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "#poke_cat_df['PC 1'] = pc[:, 0]\n",
    "#poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "#sns.scatterplot(data=poke_cat_df,\n",
    "#                x='PC 1', y='PC 2', hue='Type')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.15.svg](_images/13.15.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "#<script.py> output:\n",
    "#        Type  Legendary      PC 1      PC 2\n",
    "#    0  Grass      False -1.556375 -0.021482\n",
    "#    1  Grass      False -0.362867 -0.050269\n",
    "#    2  Grass      False  1.280152 -0.062720\n",
    "#    3  Grass      False  2.620916  0.704263\n",
    "#################################################\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "#sns.scatterplot(data=poke_cat_df,\n",
    "#                x='PC 1', y='PC 2', hue='Legendary')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.16.svg](_images/13.16.svg)\n",
    "Looks like the different types are scattered all over the place\n",
    "while the legendary Pokemon always score high for PC 1 meaning they\n",
    "have high stats overall. Their spread along the PC 2 axis tells us\n",
    "they aren't consistently fast and vulnerable or slow and armored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA in a model pipeline\n",
    "\n",
    "#We just saw that legendary Pokemon tend to have higher stats\n",
    "#overall. Let's see if we can add a classifier to our pipeline that\n",
    "#detects legendary versus non-legendary Pokemon based on the\n",
    "#principal components.\n",
    "\n",
    "#The data has been pre-loaded for you and split into training and\n",
    "#tests datasets: X_train, X_test, y_train, y_test.\n",
    "\n",
    "#Same goes for all relevant packages and classes(Pipeline(),\n",
    "#StandardScaler(), PCA(), RandomForestClassifier()).\n",
    "\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([\n",
    "#        ('scaler', StandardScaler()),\n",
    "#        ('reducer', PCA(n_components=2)),\n",
    "#        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "#pipe.fit(X_train, y_train)\n",
    "\n",
    "# Prints the explained variance ratio\n",
    "#print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.45624044 0.17767414]\n",
    "#################################################\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "#accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "#print('{0:.1%} test set accuracy'.format(accuracy))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    95.8% test set accuracy\n",
    "#################################################\n",
    "\n",
    "#Repeat the process with 3 extracted components\n",
    "# Build the pipeline\n",
    "#pipe = Pipeline([\n",
    "#        ('scaler', StandardScaler()),\n",
    "#        ('reducer', PCA(n_components=3)),\n",
    "#        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "#pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "#accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "#print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "#print('{0:.1%} test set accuracy'.format(accuracy))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0.45624044 0.17767414 0.12858833]\n",
    "#    95.0% test set accuracy\n",
    "#################################################\n",
    "# Looks like adding the third component does not increase the\n",
    "#model accuracy, even though it adds information to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Principal Component selection**\n",
    "___\n",
    "- we can set an explained variance threshold instead of number of components\n",
    "    - n_components = 0-1\n",
    "- an optimal number of components\n",
    "    - elbow plot\n",
    "- .inverse_transform goes back from PCA to original components\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Selecting the proportion of variance to keep\n",
    "\n",
    "#You'll let PCA determine the number of components to calculate\n",
    "#based on an explained variance threshold that you decide.\n",
    "\n",
    "#You'll work on the numeric ANSUR female dataset pre-loaded as\n",
    "#ansur_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded too\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Pipe a scaler to PCA selecting 80% of the variance\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=0.8))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "#print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    11 components selected\n",
    "#################################################\n",
    "\n",
    "#Increase the proportion of variance to keep to 90%\n",
    "\n",
    "# Let PCA select 90% of the variance\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=0.9))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "#print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    23 components selected\n",
    "#################################################\n",
    "#We need to more than double the components to go from 80% to 90%\n",
    "#explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Choosing the number of components\n",
    "\n",
    "#You'll now make a more informed decision on the number of principal\n",
    "#components to reduce your data to using the \"elbow in the plot\"\n",
    "#technique. One last time, you'll work on the numeric ANSUR female\n",
    "#dataset pre-loaded as ansur_df.\n",
    "\n",
    "#All relevant packages and classes have been pre-loaded for you\n",
    "#(Pipeline(), StandardScaler(), PCA()).\n",
    "\n",
    "# Pipeline a scaler and pca selecting 10 components\n",
    "#pipe = Pipeline([('scaler', StandardScaler()),\n",
    "#        \t\t ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "#pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "#plt.plot(pipe.steps[1][1].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.17.svg](_images/13.17.svg)\n",
    "The 'elbow' in the plot is at 3 components\n",
    "(the 3rd component has index 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA for image compression\n",
    "\n",
    "#You'll reduce the size of 16 images with hand written digits\n",
    "#(MNIST dataset) using PCA.\n",
    "\n",
    "#The samples are 28 by 28 pixel gray scale images that have been\n",
    "#flattened to arrays with 784 elements each (28 x 28 = 784) and\n",
    "#added to the 2D numpy array X_test. Each of the 784 pixels has a\n",
    "#value between 0 and 255 and can be regarded as a feature.\n",
    "\n",
    "#A pipeline with a scaler and PCA model to select 78 components\n",
    "#has been pre-loaded for you as pipe. This pipeline has already\n",
    "#been fitted to the entire MNIST dataset except for the 16 samples\n",
    "#in X_test.\n",
    "\n",
    "#Finally, a function plot_digits has been created for you that\n",
    "#will plot 16 images in a grid.\n",
    "\n",
    "# Plot the MNIST sample data\n",
    "#plot_digits(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.18.svg](_images/13.18.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the input data to principal components\n",
    "#pc = pipe.transform(X_test)\n",
    "\n",
    "# Prints the number of features per dataset\n",
    "#print(\"X_test has {} features\".format(X_test.shape[1]))\n",
    "#print(\"pc has {} features\".format(pc.shape[1]))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    X_test has 784 features\n",
    "#    pc has 78 features\n",
    "#################################################\n",
    "\n",
    "#Inverse transform the components back to the original feature space.\n",
    "# Inverse transform the components to original feature space\n",
    "#X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Prints the number of features\n",
    "#print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    X_rebuilt has 784 features\n",
    "#################################################\n",
    "\n",
    "# Plot the reconstructed data\n",
    "#plot_digits(X_rebuilt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/13.19.svg](_images/13.19.svg)\n",
    "You've reduced the size of the data 10 fold but were able to\n",
    "reconstruct images with reasonable quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Congratulations**\n",
    "___\n",
    "- What you've learned\n",
    "    - Why dimensionality reduction is important & when to use it\n",
    "    - Feature selection vs. extraction\n",
    "    - High dimensional data exploration with t-SNE & PCA\n",
    "    - Use models to find important features\n",
    "    - Remove unimportant features\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
