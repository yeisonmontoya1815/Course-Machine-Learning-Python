{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Cleaning Data in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 1 - Common data problems**\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency.\n",
    "\n",
    "[Link for reference](https://github.com/elmoallistair/datacamp/blob/master/cleaning-data-in-python/01_common-data-problems.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
      "0           0  1351    Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2           2  2820   Thursday        DELTA        LOS ANGELES        West US   \n",
      "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US   \n",
      "4           4  2992  Wednesday     AMERICAN              MIAMI        East US   \n",
      "5           5   634   Thursday       ALASKA             NEWARK        East US   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "5       Hub   Gates 50-59  2018-12-31     140.0  Somewhat clean   \n",
      "\n",
      "          safety        satisfaction  \n",
      "0        Neutral      Very satisfied  \n",
      "1      Very safe      Very satisfied  \n",
      "2  Somewhat safe             Neutral  \n",
      "3      Very safe  Somewhat satsified  \n",
      "4      Very safe  Somewhat satsified  \n",
      "5      Very safe      Very satisfied  \n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd \n",
    "import datetime as dt \n",
    "\n",
    "# Common path prefix\n",
    "common_path = \"C:\\\\Users\\\\yeiso\\\\OneDrive - Douglas College\\\\0. DOUGLAS COLLEGE\\\\3. Fund Machine Learning\\\\0. Python Course DataCamp\\\\Course-fundamentals-of-Machine-Learning\"\n",
    "\n",
    "# Paths for the variables with double backslashes\n",
    "airlines = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\airlines_final.csv')\n",
    "banking = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\banking_dirty.csv')\n",
    "restaurant = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\restaurants_L2.csv')\n",
    "restaurant_dirty = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\restaurants_L2_dirty.csv')\n",
    "ride_sharing = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\ride_sharing_new.csv')\n",
    "\n",
    "print(airlines.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric data or ... ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing strings and concatenating numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         duration duration_trim  duration_time\n",
      "0      12 minutes           12              12\n",
      "1      24 minutes           24              24\n",
      "2       8 minutes            8               8\n",
      "3       4 minutes            4               4\n",
      "4      11 minutes           11              11\n",
      "...           ...           ...            ...\n",
      "25755  11 minutes           11              11\n",
      "25756  10 minutes           10              10\n",
      "25757  14 minutes           14              14\n",
      "25758  14 minutes           14              14\n",
      "25759  29 minutes           29              29\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "11.389052795031056\n"
     ]
    }
   ],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tire size constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
    "\n",
    "# Save today's date\n",
    "today = pd.to_datetime(dt.date.today())\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 2 - Text and categorical data problems**\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency.\n",
    "\n",
    "[Link for reference](https://github.com/elmoallistair/datacamp/blob/master/cleaning-data-in-python/01_common-data-problems.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id       day      airline        destination    dest_region  \\\n",
      "0           0  1351   Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1           1   373    Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2           2  2820  Thursday        DELTA        LOS ANGELES        West US   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min cleanliness         safety  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0       Clean        Neutral   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0       Clean      Very safe   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0     Average  Somewhat safe   \n",
      "\n",
      "     satisfaction  \n",
      "0  Very satisfied  \n",
      "1  Very satisfied  \n",
      "2         Neutral  \n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import pandas as pd \n",
    "import datetime as dt \n",
    "\n",
    "# Common path prefix\n",
    "common_path = \"C:\\\\Users\\\\yeiso\\\\OneDrive - Douglas College\\\\0. DOUGLAS COLLEGE\\\\3. Fund Machine Learning\\\\0. Python Course DataCamp\\\\Course-fundamentals-of-Machine-Learning\"\n",
    "\n",
    "# Paths for the variables with double backslashes\n",
    "airlines = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\airlines_final.csv')\n",
    "banking = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\banking_dirty.csv')\n",
    "restaurant = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\restaurants_L2.csv')\n",
    "restaurant_dirty = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\restaurants_L2_dirty.csv')\n",
    "ride_sharing = pd.read_csv(f'{common_path}\\\\7. Cleaning Data in Python\\\\datasets\\\\ride_sharing_new.csv')\n",
    "\n",
    "print(airlines.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categories DataFrame\n",
    "print(\"categories: \")\n",
    "print(\"***********************************************************\")\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding consistency 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding consistency 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconsistent categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remapping categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing titles and taking names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping it descriptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 3 - Advanced data problems**\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses.\n",
    "\n",
    "[Link for reference](https://github.com/elmoallistair/datacamp/blob/master/cleaning-data-in-python/01_common-data-problems.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cust_id  birth_date  Age  acct_amount  inv_amount   fund_A  \\\n",
      "0           0  870A9281  1962-06-09   58     63523.31       51295  30105.0   \n",
      "1           1  166B05B0  1962-12-16   58     38175.46       15050   4995.0   \n",
      "\n",
      "   fund_B  fund_C   fund_D account_opened last_transaction  \n",
      "0  4138.0  1420.0  15632.0       02-09-18         22-02-19  \n",
      "1   938.0  6696.0   2421.0       28-02-19         31-10-18  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "banking = pd.read_csv('banking_dirty.csv')\n",
    "restaurant = pd.read_csv('restaurants_L2.csv')\n",
    "restaurant_dirty = pd.read_csv('restaurants_L2_dirty.csv')\n",
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "\n",
    "print(banking.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform currencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "#banking.loc[banking['acct_cur']=='euro', 'acct_amount'] = banking.loc[banking['acct_cur']=='euro', 'acct_amount'] * 1.1\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's our data integrity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's our data integrity?  2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing investors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values('age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the money\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount']*5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 4 - Record linkage**\n",
    "\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset. Other references. \n",
    "\n",
    "[Link for reference](https://github.com/elmoallistair/datacamp/blob/master/cleaning-data-in-python/01_common-data-problems.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     name                      addr         city       phone  \\\n",
      "0           0   kokomo         6333 w. third st.           la  2139330773   \n",
      "1           1   feenix   8358 sunset blvd. west     hollywood  2138486677   \n",
      "2           2  parkway      510 s. arroyo pkwy .     pasadena  8187951001   \n",
      "3           3     r-23          923 e. third st.  los angeles  2136877178   \n",
      "\n",
      "          type  \n",
      "0     american  \n",
      "1     american  \n",
      "2  californian  \n",
      "3     japanese  \n"
     ]
    }
   ],
   "source": [
    "#import last modification\n",
    "import pandas as pd\n",
    "# import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "banking = pd.read_csv('banking_dirty.csv')\n",
    "restaurant = pd.read_csv('restaurants_L2.csv')\n",
    "restaurant_dirty = pd.read_csv('restaurants_L2_dirty.csv')\n",
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "\n",
    "print(restaurant_dirty.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cutoff point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('indonesian', 80), ('californian', 68), ('italian', 67), ('russian', 67), ('american', 62), ('japanese', 54), ('mexican/tex-mex', 54), ('american ( new )', 54), ('mexican', 50), ('fast food', 45), ('middle eastern', 43), ('steakhouses', 40), ('pacific new wave', 40), ('pizza', 40), ('diners', 36), ('cajun/creole', 36), ('vietnamese', 36), ('continental', 36), ('seafood', 33), ('chicken', 33), ('chinese', 33), ('hot dogs', 30), ('hamburgers', 30), ('coffee shops', 30), ('noodle shops', 30), ('southern/soul', 30), ('desserts', 30), ('eclectic', 26), ('coffeebar', 26), ('health food', 22), ('french ( new )', 22), ('delis', 20)]\n",
      "[('american', 100), ('american ( new )', 90), ('mexican', 80), ('mexican/tex-mex', 72), ('asian', 62), ('italian', 53), ('russian', 53), ('californian', 53), ('middle eastern', 51), ('southern/soul', 47), ('pacific new wave', 45), ('hamburgers', 44), ('indonesian', 44), ('cajun/creole', 42), ('chicken', 40), ('pizza', 40), ('japanese', 38), ('eclectic', 38), ('delis', 36), ('french ( new )', 34), ('vietnamese', 33), ('diners', 29), ('seafood', 27), ('chinese', 27), ('desserts', 25), ('coffeebar', 24), ('steakhouses', 21), ('health food', 21), ('continental', 21), ('coffee shops', 20), ('noodle shops', 20), ('fast food', 12), ('hot dogs', 0)]\n",
      "[('italian', 100), ('asian', 67), ('californian', 56), ('continental', 54), ('american', 53), ('indonesian', 47), ('russian', 43), ('mexican', 43), ('japanese', 40), ('mexican/tex-mex', 39), ('american ( new )', 39), ('pacific new wave', 39), ('middle eastern', 38), ('vietnamese', 35), ('delis', 33), ('pizza', 33), ('steakhouses', 33), ('health food', 33), ('diners', 31), ('cajun/creole', 30), ('chicken', 29), ('chinese', 29), ('southern/soul', 28), ('eclectic', 27), ('noodle shops', 22), ('french ( new )', 18), ('seafood', 14), ('hot dogs', 13), ('desserts', 13), ('fast food', 12), ('coffeebar', 12), ('hamburgers', 12), ('coffee shops', 0)]\n"
     ]
    }
   ],
   "source": [
    "#install\n",
    "        #pip install python-Levenshtein thefuzz\n",
    "#import\n",
    "        #from thefuzz import process\n",
    "\n",
    "\n",
    "\n",
    "# Import process from thefuzz\n",
    "from thefuzz import process #this name \"the fuzz\" was given randonmly in the course!!!!!\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurant_dirty['type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remapping categories II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'californian' 'japanese' 'cajun/creole' 'hot dogs' 'diners'\n",
      " 'delis' 'hamburgers' 'seafood' 'italian' 'coffee shops' 'russian'\n",
      " 'steakhouses' 'mexican/tex-mex' 'noodle shops' 'mexican' 'middle eastern'\n",
      " 'asian' 'vietnamese' 'health food' 'american ( new )' 'pacific new wave'\n",
      " 'indonesian' 'eclectic' 'chicken' 'fast food' 'southern/soul' 'coffeebar'\n",
      " 'continental' 'french ( new )' 'desserts' 'chinese' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurant_dirty['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italian', 100, 14), ('italian', 100, 21), ('italian', 100, 47), ('italian', 100, 57), ('italian', 100, 73)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurant_dirty['type'], limit = len(restaurant_dirty))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurant_dirty['type'], limit=len(restaurant_dirty))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1]>=80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurant_dirty.loc[restaurant_dirty['type'] == match[0], 'type'] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeiso\\AppData\\Local\\Temp\\ipykernel_38696\\3801345375.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Asian' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  restaurant_dirty.loc[restaurant_dirty['type'] == match[0]] = cuisine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Mexican', 'californian', 'japanese', 'cajun/creole', 'hot dogs',\n",
       "       'diners', 'delis', 'hamburgers', 'seafood', 'Italian',\n",
       "       'coffee shops', 'russian', 'steakhouses', 'noodle shops',\n",
       "       'middle eastern', 'Asian', 'vietnamese', 'health food',\n",
       "       'pacific new wave', 'eclectic', 'chicken', 'fast food',\n",
       "       'southern/soul', 'coffeebar', 'continental', 'French', 'desserts',\n",
       "       'chinese', 'pizza'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List of predefined categories\n",
    "categories = ['Asian', 'American', 'Italian', 'Mexican', 'French']\n",
    "\n",
    "\n",
    "# Iterate through categories\n",
    "for cuisine in categories:\n",
    "    # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "    matches = process.extract(cuisine, restaurant_dirty['type'], limit=len(restaurant_dirty.type))\n",
    "\n",
    "    # Iterate through the list of matches\n",
    "    for match in matches:\n",
    "        # Check whether the similarity score is greater than or equal to 80\n",
    "        if match[1] >= 80:\n",
    "            # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "            restaurant_dirty.loc[restaurant_dirty['type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result\n",
    "restaurant_dirty['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install this package\n",
    "#pip install recordlinkage\n",
    "\n",
    "# Import the required library\n",
    "import recordlinkage\n",
    "\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurant_dirty, restaurant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Find exact matches on city, cuisine_types - \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurant_dirty, restaurant)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking them together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurant_dirty[~restaurant_dirty.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurant.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
