{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **23. Ensemble Methods in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 - Combining Multiple Models\n",
    "\n",
    "Do you struggle to determine which of the models you built is the best for your problem? You should give up on that, and use them all instead! In this chapter, you'll learn how to combine multiple models into one using \"Voting\" and \"Averaging\". You'll use these to predict the ratings of apps on the Google Play Store, whether or not a Pokémon is legendary, and which characters are going to die in Game of Thrones!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Google apps data\n",
    "\n",
    "The first dataset you'll work with contains information about the ratings of around 10,800 apps on the Google Play store. It has been preloaded into a DataFrame called ratings. There are 13 features that describe a given app, such as 'Category' and 'Price'. The goal is to use this information to predict the rating of an app, which can range from 1 to 5.\n",
    "\n",
    "Before doing that, it's a good idea to familiarize yourself with the dataset using pandas methods such as .head() and .describe(). Explore the data in the IPython Shell and select the incorrect statement from the options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Category</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Size</th>\n",
       "      <th>Installs</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Content Rating</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Last Updated</th>\n",
       "      <th>Current Ver</th>\n",
       "      <th>Android Ver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.1</td>\n",
       "      <td>159</td>\n",
       "      <td>19M</td>\n",
       "      <td>10,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Art &amp; Design</td>\n",
       "      <td>January 7, 2018</td>\n",
       "      <td>1.0.0</td>\n",
       "      <td>4.0.3 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coloring book moana</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>967</td>\n",
       "      <td>14M</td>\n",
       "      <td>500,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Art &amp; Design;Pretend Play</td>\n",
       "      <td>January 15, 2018</td>\n",
       "      <td>2.0.0</td>\n",
       "      <td>4.0.3 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U Launcher Lite – FREE Live Cool Themes, Hide ...</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>87510</td>\n",
       "      <td>8.7M</td>\n",
       "      <td>5,000,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Art &amp; Design</td>\n",
       "      <td>August 1, 2018</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>4.0.3 and up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 App        Category  Rating  \\\n",
       "0     Photo Editor & Candy Camera & Grid & ScrapBook  ART_AND_DESIGN     4.1   \n",
       "1                                Coloring book moana  ART_AND_DESIGN     3.9   \n",
       "2  U Launcher Lite – FREE Live Cool Themes, Hide ...  ART_AND_DESIGN     4.7   \n",
       "\n",
       "  Reviews  Size    Installs  Type Price Content Rating  \\\n",
       "0     159   19M     10,000+  Free     0       Everyone   \n",
       "1     967   14M    500,000+  Free     0       Everyone   \n",
       "2   87510  8.7M  5,000,000+  Free     0       Everyone   \n",
       "\n",
       "                      Genres      Last Updated Current Ver   Android Ver  \n",
       "0               Art & Design   January 7, 2018       1.0.0  4.0.3 and up  \n",
       "1  Art & Design;Pretend Play  January 15, 2018       2.0.0  4.0.3 and up  \n",
       "2               Art & Design    August 1, 2018       1.2.4  4.0.3 and up  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.193338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.537431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating\n",
       "count  9367.000000\n",
       "mean      4.193338\n",
       "std       0.537431\n",
       "min       1.000000\n",
       "25%       4.000000\n",
       "50%       4.300000\n",
       "75%       4.500000\n",
       "max      19.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type',\n",
       "       'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver',\n",
       "       'Android Ver'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the rating of an app\n",
    "\n",
    "Having explored the Google apps dataset in the previous exercise, let's now build a model that predicts the rating of an app given a subset of its features.\n",
    "\n",
    "To do this, you'll use scikit-learn's DecisionTreeRegressor. As decision trees are the building blocks of many ensemble models, refreshing your memory of how they work will serve you well throughout this course.\n",
    "\n",
    "We'll use the MAE (mean absolute error) as the evaluation metric. This metric is highly interpretable, as it represents the average absolute difference between actual and predicted ratings.\n",
    "\n",
    "All required modules have been pre-imported for you. The features and target are available in the variables X and y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('googleplaystore.csv')\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df[['Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating']]\n",
    "y = df['Rating']\n",
    "\n",
    "# Split into train (80%) and test(20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the regressor\n",
    "reg_dt = DecisionTreeRegressor(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "\n",
    "# Fit to the training set\n",
    "reg_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = reg_dt.predict(X_test)\n",
    "print('MAE: {:.3f}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     MAE: 0.609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best model\n",
    "\n",
    "In this exercise, you'll compare different classifiers and choose the one that performs the best.\n",
    "\n",
    "The dataset here - already loaded and split into train and test sets - consists of Pokémon - their stats, types, and whether or not they're legendary. The objective of our classifiers is to predict this 'Legendary' variable.\n",
    "\n",
    "Three individual classifiers have been fitted to the training set:\n",
    "\n",
    "clf_lr is a logistic regression.\n",
    "clf_dt is a decision tree.\n",
    "clf_knn is a 5-nearest neighbors classifier.\n",
    "As the classes here are imbalanced - only 65 of the 800 Pokémon in the dataset are legendary - we'll use F1-Score to evaluate the performance. Scikit-learn's f1_score() has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define and instantiate classifiers\n",
    "clf_lr = LogisticRegression()\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Make the individual predictions\n",
    "pred_lr = clf_lr.predict(X_test)\n",
    "pred_dt = clf_dt.predict(X_test)\n",
    "pred_knn = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of each model\n",
    "score_lr = f1_score(y_test, pred_lr)\n",
    "score_dt = f1_score(y_test, pred_dt)\n",
    "score_knn = f1_score(y_test, pred_knn)\n",
    "\n",
    "# Print the scores\n",
    "print(score_lr)\n",
    "print(score_dt)\n",
    "print(score_knn)\n",
    "\n",
    "\n",
    "\n",
    "script.py\n",
    "123456789101112131415\n",
    "# Make the invidual predictions\n",
    "pred_lr = clf_lr.predict(X_test)\n",
    "pred_dt = clf_dt.predict(X_test)\n",
    "pred_knn = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of each model\n",
    "score_lr = f1_score(y_test, pred_lr)\n",
    "score_dt = f1_score(y_test, pred_dt)\n",
    "score_knn = f1_score(y_test, pred_knn)\n",
    "\n",
    "IPython Shell\n",
    "Slides\n",
    "# Make the invidual predictions\n",
    "pred_lr = clf_lr.predict(X_test)\n",
    "pred_dt = clf_dt.predict(X_test)\n",
    "pred_knn = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of each model\n",
    "score_lr = f1_score(y_test, pred_lr)\n",
    "score_dt = f1_score(y_test, pred_dt)\n",
    "score_knn = f1_score(y_test, pred_knn)\n",
    "\n",
    "\n",
    "# Print the scores\n",
    "print(score_lr)\n",
    "print(score_dt)\n",
    "print(score_knn)\n",
    "\n",
    "\n",
    "# Logistic regression (clf_lr).\n",
    "# Decision tree (clf_dt).\n",
    "# 5-nearest neighbors (clf_knn).\n",
    "\n",
    "# <script.py> output:\n",
    "    # 0.5882352941176471\n",
    "    # 0.5833333333333334\n",
    "    # 0.47619047619047616\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling your first ensemble\n",
    "\n",
    "It's time to build your first ensemble model! The Pokémon dataset from the previous exercise has been loaded and split into train and test sets.\n",
    "\n",
    "Your job is to leverage the voting ensemble technique using the sklearn API. It's up to you to instantiate the individual models and pass them as parameters to build your first voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate the individual models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_lr = LogisticRegression(class_weight='balanced')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "\n",
    "# Create and fit the voting classifier\n",
    "clf_vote = VotingClassifier(\n",
    "    estimators=[('knn', clf_knn), ('lr', clf_lr), ('dt', clf_dt)]\n",
    ")\n",
    "clf_vote.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating your ensemble\n",
    "\n",
    "In the previous exercise, you built your first voting classifier. Let's now evaluate it and compare it to that of the individual models.\n",
    "\n",
    "The individual models (clf_knn, clf_dt, and clf_lr) and the voting classifier (clf_vote) have already been loaded and trained.\n",
    "\n",
    "Remember to use f1_score() to evaluate the performance. In addition, you'll create a classification report on the test set (X_test, y_test) using the classification_report() function.\n",
    "\n",
    "Will your voting classifier beat the 58% F1-score of the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Calculate the predictions using the voting classifier\n",
    "pred_vote = clf_vote.predict(X_test)\n",
    "\n",
    "# Calculate the F1-Score of the voting classifier\n",
    "score_vote = f1_score(y_test, pred_vote)\n",
    "print('F1-Score: {:.3f}'.format(score_vote))\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test, pred_vote)\n",
    "print(report)\n",
    "\n",
    "# <script.py> output:\n",
    "#     F1-Score: 0.583\n",
    "#                   precision    recall  f1-score   support\n",
    "    \n",
    "#            False       0.98      0.95      0.97       150\n",
    "#             True       0.50      0.70      0.58        10\n",
    "    \n",
    "#         accuracy                           0.94       160\n",
    "#        macro avg       0.74      0.83      0.77       160\n",
    "#     weighted avg       0.95      0.94      0.94       160\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journey to Westeros\n",
    "\n",
    "If you're a Game of Thrones fan, you might already know all about the fictional world of Westeros and the characters that inhabit it. Regardless, it's important to explore a new dataset before doing any modeling. That's what you'll do now!\n",
    "\n",
    "The dataset is loaded into the environment and available to you as got, the commonly used acronym for Game of Thrones.\n",
    "\n",
    "The target variable here is 'actual'. It represents whether a character is alive (1) or not (0). First explore the target using the .describe() method. What can you conclude about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>actual</th>\n",
       "      <th>pred</th>\n",
       "      <th>alive</th>\n",
       "      <th>plod</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>male</th>\n",
       "      <th>culture</th>\n",
       "      <th>dateOfBirth</th>\n",
       "      <th>...</th>\n",
       "      <th>isAliveHeir</th>\n",
       "      <th>isAliveSpouse</th>\n",
       "      <th>isMarried</th>\n",
       "      <th>isNoble</th>\n",
       "      <th>age</th>\n",
       "      <th>numDeadRelations</th>\n",
       "      <th>boolDeadRelations</th>\n",
       "      <th>isPopular</th>\n",
       "      <th>popularity</th>\n",
       "      <th>isAlive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.946</td>\n",
       "      <td>Viserys II Targaryen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.613</td>\n",
       "      <td>Walder Frey</td>\n",
       "      <td>Lord of the Crossing</td>\n",
       "      <td>1</td>\n",
       "      <td>Rivermen</td>\n",
       "      <td>208.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.507</td>\n",
       "      <td>Addison Hill</td>\n",
       "      <td>Ser</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.No  actual  pred  alive   plod                  name  \\\n",
       "0     1       0     0  0.054  0.946  Viserys II Targaryen   \n",
       "1     2       1     0  0.387  0.613           Walder Frey   \n",
       "2     3       1     0  0.493  0.507          Addison Hill   \n",
       "\n",
       "                  title  male   culture  dateOfBirth  ...  isAliveHeir  \\\n",
       "0                   NaN     1       NaN          NaN  ...          0.0   \n",
       "1  Lord of the Crossing     1  Rivermen        208.0  ...          NaN   \n",
       "2                   Ser     1       NaN          NaN  ...          NaN   \n",
       "\n",
       "  isAliveSpouse isMarried isNoble   age numDeadRelations  boolDeadRelations  \\\n",
       "0           NaN         0       0   NaN               11                  1   \n",
       "1           1.0         1       1  97.0                1                  1   \n",
       "2           NaN         0       1   NaN                0                  0   \n",
       "\n",
       "   isPopular  popularity  isAlive  \n",
       "0          1    0.605351        0  \n",
       "1          1    0.896321        1  \n",
       "2          0    0.267559        1  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('character-predictions.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S.No', 'actual', 'pred', 'alive', 'plod', 'name', 'title', 'male',\n",
       "       'culture', 'dateOfBirth', 'DateoFdeath', 'mother', 'father', 'heir',\n",
       "       'house', 'spouse', 'book1', 'book2', 'book3', 'book4', 'book5',\n",
       "       'isAliveMother', 'isAliveFather', 'isAliveHeir', 'isAliveSpouse',\n",
       "       'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations',\n",
       "       'isPopular', 'popularity', 'isAlive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>actual</th>\n",
       "      <th>pred</th>\n",
       "      <th>alive</th>\n",
       "      <th>plod</th>\n",
       "      <th>male</th>\n",
       "      <th>dateOfBirth</th>\n",
       "      <th>DateoFdeath</th>\n",
       "      <th>book1</th>\n",
       "      <th>book2</th>\n",
       "      <th>...</th>\n",
       "      <th>isAliveHeir</th>\n",
       "      <th>isAliveSpouse</th>\n",
       "      <th>isMarried</th>\n",
       "      <th>isNoble</th>\n",
       "      <th>age</th>\n",
       "      <th>numDeadRelations</th>\n",
       "      <th>boolDeadRelations</th>\n",
       "      <th>isPopular</th>\n",
       "      <th>popularity</th>\n",
       "      <th>isAlive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>973.500000</td>\n",
       "      <td>0.745632</td>\n",
       "      <td>0.687050</td>\n",
       "      <td>0.634470</td>\n",
       "      <td>0.365530</td>\n",
       "      <td>0.619219</td>\n",
       "      <td>1577.364896</td>\n",
       "      <td>2950.193694</td>\n",
       "      <td>0.198356</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.778986</td>\n",
       "      <td>0.141829</td>\n",
       "      <td>0.460946</td>\n",
       "      <td>-1293.563510</td>\n",
       "      <td>0.305755</td>\n",
       "      <td>0.074512</td>\n",
       "      <td>0.059096</td>\n",
       "      <td>0.089584</td>\n",
       "      <td>0.745632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>561.906131</td>\n",
       "      <td>0.435617</td>\n",
       "      <td>0.463813</td>\n",
       "      <td>0.312637</td>\n",
       "      <td>0.312637</td>\n",
       "      <td>0.485704</td>\n",
       "      <td>19565.414460</td>\n",
       "      <td>28192.245529</td>\n",
       "      <td>0.398864</td>\n",
       "      <td>0.484148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486985</td>\n",
       "      <td>0.415684</td>\n",
       "      <td>0.348965</td>\n",
       "      <td>0.498601</td>\n",
       "      <td>19564.340993</td>\n",
       "      <td>1.383910</td>\n",
       "      <td>0.262669</td>\n",
       "      <td>0.235864</td>\n",
       "      <td>0.160568</td>\n",
       "      <td>0.435617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-298001.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>487.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391250</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013378</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>973.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735500</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1459.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>298299.000000</td>\n",
       "      <td>298299.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No       actual         pred        alive         plod  \\\n",
       "count  1946.000000  1946.000000  1946.000000  1946.000000  1946.000000   \n",
       "mean    973.500000     0.745632     0.687050     0.634470     0.365530   \n",
       "std     561.906131     0.435617     0.463813     0.312637     0.312637   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     487.250000     0.000000     0.000000     0.391250     0.101000   \n",
       "50%     973.500000     1.000000     1.000000     0.735500     0.264500   \n",
       "75%    1459.750000     1.000000     1.000000     0.899000     0.608750   \n",
       "max    1946.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              male    dateOfBirth    DateoFdeath        book1        book2  \\\n",
       "count  1946.000000     433.000000     444.000000  1946.000000  1946.000000   \n",
       "mean      0.619219    1577.364896    2950.193694     0.198356     0.374615   \n",
       "std       0.485704   19565.414460   28192.245529     0.398864     0.484148   \n",
       "min       0.000000     -28.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     240.000000     282.000000     0.000000     0.000000   \n",
       "50%       1.000000     268.000000     299.000000     0.000000     0.000000   \n",
       "75%       1.000000     285.000000     299.000000     0.000000     1.000000   \n",
       "max       1.000000  298299.000000  298299.000000     1.000000     1.000000   \n",
       "\n",
       "       ...  isAliveHeir  isAliveSpouse    isMarried      isNoble  \\\n",
       "count  ...    23.000000     276.000000  1946.000000  1946.000000   \n",
       "mean   ...     0.652174       0.778986     0.141829     0.460946   \n",
       "std    ...     0.486985       0.415684     0.348965     0.498601   \n",
       "min    ...     0.000000       0.000000     0.000000     0.000000   \n",
       "25%    ...     0.000000       1.000000     0.000000     0.000000   \n",
       "50%    ...     1.000000       1.000000     0.000000     0.000000   \n",
       "75%    ...     1.000000       1.000000     0.000000     1.000000   \n",
       "max    ...     1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "                 age  numDeadRelations  boolDeadRelations    isPopular  \\\n",
       "count     433.000000       1946.000000        1946.000000  1946.000000   \n",
       "mean    -1293.563510          0.305755           0.074512     0.059096   \n",
       "std     19564.340993          1.383910           0.262669     0.235864   \n",
       "min   -298001.000000          0.000000           0.000000     0.000000   \n",
       "25%        18.000000          0.000000           0.000000     0.000000   \n",
       "50%        27.000000          0.000000           0.000000     0.000000   \n",
       "75%        50.000000          0.000000           0.000000     0.000000   \n",
       "max       100.000000         15.000000           1.000000     1.000000   \n",
       "\n",
       "        popularity      isAlive  \n",
       "count  1946.000000  1946.000000  \n",
       "mean      0.089584     0.745632  \n",
       "std       0.160568     0.435617  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.013378     0.000000  \n",
       "50%       0.033445     1.000000  \n",
       "75%       0.086957     1.000000  \n",
       "max       1.000000     1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1946.000000\n",
       "mean        0.745632\n",
       "std         0.435617\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"actual\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting GoT deaths\n",
    "\n",
    "While the target variable does not have any missing values, other features do. As the focus of the course is not on data cleaning and preprocessing, we have already done the following preprocessing for you:\n",
    "\n",
    "Replaced NA values with 0.\n",
    "Replace negative values of age with 0.\n",
    "Replace NA values of age with the mean.\n",
    "Let's now build an ensemble model using the averaging technique. The following individual models have been built:\n",
    "\n",
    "Logistic Regression (clf_lr).\n",
    "Decision Tree (clf_dt).\n",
    "Support Vector Machine (clf_svm).\n",
    "As the target is binary, all these models might have good individual performance. Your objective is to combine them using averaging. Recall from the video that this is the same as a soft voting approach, so you should still use the VotingClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build the individual models\n",
    "clf_lr = LogisticRegression(class_weight='balanced')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf_svm = SVC(probability=True, class_weight='balanced', random_state=500)\n",
    "\n",
    "# List of (string, estimator) tuples\n",
    "estimators = [('lr', clf_lr), ('dt', clf_dt), ('svm', clf_svm)]\n",
    "\n",
    "# Build and fit an averaging classifier\n",
    "clf_avg = VotingClassifier(estimators=estimators, voting='soft')\n",
    "clf_avg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "acc_avg = accuracy_score(y_test,  clf_avg.predict(X_test))\n",
    "print('Accuracy: {:.2f}'.format(acc_avg))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy: 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft vs. hard voting\n",
    "\n",
    "You've now practiced building two types of ensemble methods: Voting and Averaging (soft voting). Which one is better? It's best to try both of them and then compare their performance. Let's try this now using the Game of Thrones dataset.\n",
    "\n",
    "Three individual classifiers have been instantiated for you:\n",
    "\n",
    "A DecisionTreeClassifier (clf_dt).\n",
    "A LogisticRegression (clf_lr).\n",
    "A KNeighborsClassifier (clf_knn).\n",
    "Your task is to try both voting and averaging to determine which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# List of (string, estimator) tuples\n",
    "estimators = [('DecisionTree', clf_dt), ('LogisticRegression', clf_lr), ('KNeighbors', clf_knn)]\n",
    "\n",
    "# Build and fit a voting classifier\n",
    "clf_vote = VotingClassifier(estimators=estimators, voting='hard')\n",
    "clf_vote.fit(X_train, y_train)\n",
    "\n",
    "# Build and fit an averaging classifier\n",
    "clf_avg = VotingClassifier(estimators=estimators, voting='soft')\n",
    "clf_avg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of both models\n",
    "acc_vote = accuracy_score(y_test, clf_vote.predict(X_test))\n",
    "acc_avg = accuracy_score(y_test, clf_avg.predict(X_test))\n",
    "print('Voting: {:.2f}, Averaging: {:.2f}'.format(acc_vote, acc_avg))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Voting: 0.80, Averaging: 0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 - Bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted and unrestricted decision trees\n",
    "\n",
    "For this exercise, we will revisit the Pokémon dataset from the last chapter. Recall that the goal is to predict whether or not a given Pokémon is legendary.\n",
    "\n",
    "Here, you will build two separate decision tree classifiers. In the first, you will specify the parameters min_samples_leaf and min_samples_split, but not a maximum depth, so that the tree can fully develop without any restrictions.\n",
    "\n",
    "In the second, you will specify some constraints by limiting the depth of the decision tree. By then comparing the two models, you'll better understand the notion of a \"weak\" learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrestricted Decision Tree Accuracy: 0.9625\n",
      "Restricted Decision Tree Accuracy: 0.9625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Pokemon.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['Name', 'Type 1', 'Type 2', 'Legendary'])\n",
    "y = df['Legendary']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the first decision tree classifier (unrestricted)\n",
    "clf_unrestricted = DecisionTreeClassifier(min_samples_leaf=1, min_samples_split=2)\n",
    "clf_unrestricted.fit(X_train, y_train)\n",
    "\n",
    "# Build the second decision tree classifier (restricted)\n",
    "clf_restricted = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1, min_samples_split=2)\n",
    "clf_restricted.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred_unrestricted = clf_unrestricted.predict(X_test)\n",
    "acc_unrestricted = accuracy_score(y_test, y_pred_unrestricted)\n",
    "\n",
    "y_pred_restricted = clf_restricted.predict(X_test)\n",
    "acc_restricted = accuracy_score(y_test, y_pred_restricted)\n",
    "\n",
    "print(\"Unrestricted Decision Tree Accuracy:\", acc_unrestricted)\n",
    "print(\"Restricted Decision Tree Accuracy:\", acc_restricted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[146   4]\n",
      " [  1   9]]\n",
      "F1-Score: 0.783\n"
     ]
    }
   ],
   "source": [
    "# Build unrestricted decision tree\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# Print the F1 score\n",
    "score = f1_score(y_test, pred)\n",
    "print('F1-Score: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[147   3]\n",
      " [  0  10]]\n",
      "F1-Score: 0.870\n"
     ]
    }
   ],
   "source": [
    "# Build restricted decision tree\n",
    "clf = DecisionTreeClassifier(max_depth=4, max_features=2, random_state=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# Print the F1 score\n",
    "score = f1_score(y_test, pred)\n",
    "print('F1-Score: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with bootstrapping\n",
    "\n",
    "Let's now build a \"weak\" decision tree classifier and train it on a sample of the training set drawn with replacement. This will help you understand what happens on every iteration of a bagging ensemble.\n",
    "\n",
    "To take a sample, you'll use pandas' .sample() method, which has a replace parameter. For example, the following line of code samples with replacement from the whole DataFrame df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=4, random_state=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=4, random_state=500)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=4, random_state=500)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Take a sample with replacement\n",
    "X_train_sample = X_train.sample(frac=1, replace=True, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "# Build a \"weak\" Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=500, max_depth=4)\n",
    "\n",
    "# Fit the model to the training sample\n",
    "clf.fit(X_train_sample, y_train_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first attempt at bagging\n",
    "\n",
    "You've seen what happens in a single iteration of a bagging ensemble. Now let's build a custom bagging model!\n",
    "\n",
    "Two functions have been prepared for you:\n",
    "\n",
    "    def build_decision_tree(X_train, y_train, random_state=None):\n",
    "        # Takes a sample with replacement,\n",
    "        # builds a \"weak\" decision tree,\n",
    "        # and fits it to the train set\n",
    "\n",
    "    def predict_voting(classifiers, X_test):\n",
    "        # Makes the individual predictions \n",
    "        # and then combines them using \"Voting\"\n",
    "\n",
    "Technically, the build_decision_tree() function is what you did in the previous exercise. Here, you will build multiple such trees and then combine them. Let's see if this ensemble of \"weak\" models improves performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the list of individual models\n",
    "clf_list = []\n",
    "for i in range(21):\n",
    "\tweak_dt = build_decision_tree(X_train, y_train, random_state=i)\n",
    "\tclf_list.append(weak_dt)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = predict_voting(clf_list, X_test)\n",
    "\n",
    "# Print the F1 score\n",
    "print('F1 score: {:.3f}'.format(f1_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging: the scikit-learn way\n",
    "\n",
    "Let's now apply scikit-learn's BaggingClassifier to the Pokémon dataset.\n",
    "\n",
    "You obtained an F1 score of around 0.63 with your custom bagging ensemble.\n",
    "\n",
    "Will BaggingClassifier() beat it? Time to find out!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate the base model, clf_dt: a \"restricted\" decision tree with a max depth of 4.\n",
    "Build a bagging classifier with the decision tree as base estimator, using 21 estimators.\n",
    "Predict the labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Instantiate the base model\n",
    "clf_dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Build the Bagging classifier\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_dt, n_estimators=21, random_state=500)\n",
    "\n",
    "# Fit the Bagging model to the training set\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "pred = clf_bag.predict(X_test)\n",
    "\n",
    "# Show the F1-score\n",
    "print('F1-Score: {:.3f}'.format(f1_score(y_test, pred)))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     F1-Score: 0.667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the out-of-bag score\n",
    "\n",
    "\n",
    "Let's now check the out-of-bag score for the model from the previous exercise.\n",
    "\n",
    "So far you've used the F1 score to measure performance. However, in this exercise you should use the accuracy score so that you can easily compare it to the out-of-bag score.\n",
    "\n",
    "The decision tree classifier from the previous exercise, clf_dt, is available in your workspace.\n",
    "\n",
    "The pokemon dataset is already loaded for you and split into train and test sets. In addition, the decision tree classifier was fit and is available for you as clf_dt to use it as base estimator.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Build the bagging classifier using the decision tree as base estimator and 21 estimators. This time, use the out-of-bag score by specifying an argument for the oob_score parameter.\n",
    "Print the classifier's out-of-bag score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build and train the bagging classifier\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_dt, n_estimators=21, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Print the out-of-bag score\n",
    "print('OOB-Score: {:.3f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Evaluate the performance on the test set to compare\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex bagging model\n",
    "\n",
    "Having explored the semi-conductor data, let's now build a bagging classifier to predict the 'Pass/Fail' label given the input features.\n",
    "\n",
    "The preprocessed dataset is available in your workspace as uci_secom, and training and test sets have been created for you.\n",
    "\n",
    "As the target has a high class imbalance, use a \"balanced\" logistic regression as the base estimator here.\n",
    "\n",
    "We will also reduce the computation time for LogisticRegression with the parameter solver='liblinear', which is a faster optimizer than the default.\n",
    "\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "\n",
    "Instantiate a logistic regression to use as the base classifier with the parameters: class_weight='balanced', solver='liblinear', and random_state=42.\n",
    "Build a bagging classifier using the logistic regression as the base estimator, specifying the maximum number of features as 10, and including the out-of-bag score.\n",
    "Print the out-of-bag score to compare to the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Build a balanced logistic regression\n",
    "clf_lr = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_lr, max_features=10, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set and show the out-of-bag score\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, pred)))\n",
    "print('OOB-Score: {:.2f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy:  0.71\n",
    "#     OOB-Score: 0.60\n",
    "#     [[423 162]\n",
    "#      [ 20  22]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning bagging hyperparameters\n",
    "\n",
    "While you can easily build a bagging classifier using the default parameters, it is highly recommended that you tune these in order to achieve optimal performance. Ideally, these should be optimized using K-fold cross-validation.\n",
    "\n",
    "In this exercise, let's see if we can improve model performance by modifying the parameters of the bagging classifier.\n",
    "\n",
    "Here we are also passing the parameter solver='liblinear' to LogisticRegression to reduce the computation time.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build a bagging classifier using as base the logistic regression, with 20 base estimators, 10 maximum features, 0.65 (65%) maximum samples (max_samples), and sample without replacement.\n",
    "Use clf_bag to predict the labels of the test set, X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Build a balanced logistic regression\n",
    "clf_base = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier with custom parameters\n",
    "clf_bag = BaggingClassifier(base_estimator=clf_base, n_estimators=20, max_features=10, max_samples=0.65, bootstrap=False, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Calculate predictions and evaluate the accuracy on the test set\n",
    "y_pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy:  0.72\n",
    "#                   precision    recall  f1-score   support\n",
    "    \n",
    "#               -1       0.95      0.74      0.83       585\n",
    "#                1       0.11      0.43      0.17        42\n",
    "    \n",
    "#         accuracy                           0.72       627\n",
    "#        macro avg       0.53      0.59      0.50       627\n",
    "#     weighted avg       0.89      0.72      0.79       627\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 3 - Boosting**\n",
    "\n",
    "Boosting is class of ensemble learning algorithms that includes award-winning models such as AdaBoost. In this chapter, you'll learn about this award-winning model, and use it to predict the revenue of award-winning movies! You'll also learn about gradient boosting algorithms such as CatBoost and XGBoost.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the movie database\n",
    "\n",
    "Throughout this chapter, you'll be working with the TMDb (The Movie Database). This contains metadata on around 5000 movies.\n",
    "\n",
    "The dataset is loaded and available to you as movies.\n",
    "\n",
    "Your main objective is to predict movie revenue - more specifically, log-revenue, which is the normalized version of the revenue feature.\n",
    "\n",
    "Use the .describe() method to explore the variable target, containing the values shown in the histogram. You can also inspect the histogram to the right. What can you conclude?\n",
    "\n",
    "**Possible answers**\n",
    "\n",
    "-The average log-revenue is around 16.77.\n",
    "\n",
    "    -There are many zero values.\n",
    "\n",
    "-There are many extreme values.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting movie revenue\n",
    "\n",
    "Let's begin the challenge of predicting movie revenue by building a simple linear regression to estimate the log-revenue of movies based on the 'budget' feature. The metric you will use here is the RMSE (root mean squared error). To calculate this using scikit-learn, you can use the mean_squared_error() function from the sklearn.metrics module and then take its square root using numpy.\n",
    "\n",
    "The movies dataset has been loaded for you and split into train and test sets. Additionally, the missing values have been replaced with zeros. We also standardized the input feature by using StandardScaler(). Check out DataCamp's courses on cleaning data and feature engineering if you want to learn more about preprocessing for machine learning.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate the default LinearRegression model.\n",
    "Calculate the predictions on the test set.\n",
    "Calculate the RMSE. The mean_squared_error() function requires two arguments: y_test, followed by the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Build and fit linear regression model\n",
    "reg_lm = LinearRegression()\n",
    "reg_lm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_lm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "\n",
    "# <script.py> output:\n",
    "#     RMSE: 7.335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting for predicted revenue\n",
    "\n",
    "The initial model got an RMSE of around 7.34. Let's see if we can improve this using an iteration of boosting.\n",
    "\n",
    "You'll build another linear regression, but this time the target values are the errors from the base model, calculated as follows:\n",
    "\n",
    "y_train_error = pred_train - y_train\n",
    "y_test_error = pred_test - y_test\n",
    "For this model you'll use 'popularity' feature instead, hoping that it can provide more informative patterns than with the 'budget' feature alone. This is available to you as X_train_pop and X_test_pop. As in the previous exercise, the input features have been standardized for you.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Fit a linear regression model to the previous errors using X_train_pop and y_train_error.\n",
    "Calculate the predicted errors on the test set, X_test_pop.\n",
    "Calculate the RMSE, like in the previous exercise, using y_test_error and pred_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model to the previous errors\n",
    "reg_error = LinearRegression()\n",
    "reg_error.fit(X_train_pop, y_train_error)\n",
    "\n",
    "# Calculate the predicted errors on the test set\n",
    "pred_error = reg_error.predict(X_test_pop)\n",
    "\n",
    "# Evaluate the updated performance\n",
    "rmse_error = np.sqrt(mean_squared_error(y_test_error, pred_error))\n",
    "print('RMSE: {:.3f}'.format(rmse_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first AdaBoost model\n",
    "\n",
    "In the previous lesson you built models to predict the log-revenue of movies. You started with a simple linear regression and got an RMSE of 7.34. Then, you tried to improve it with an iteration of boosting, getting to a lower RMSE of 7.28.\n",
    "\n",
    "In this exercise, you'll build your first AdaBoost model - an AdaBoostRegressor - in an attempt to improve performance even further.\n",
    "\n",
    "The movies dataset has been loaded and split into train and test sets. Here you'll be using the 'budget' and 'popularity' features, which were already standardized for you using StandardScaler() from sklearn.preprocessing module.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate the default linear regression model.\n",
    "Build and fit an AdaBoostRegressor, using the linear regression as the base model and 12 estimators.\n",
    "Calculate the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the default linear regression model\n",
    "reg_lm = LinearRegression()\n",
    "\n",
    "# Build and fit an AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(base_estimator=reg_lm, n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     RMSE: 7.179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based AdaBoost regression\n",
    "\n",
    "AdaBoost models are usually built with decision trees as the base estimators. Let's give this a try now and see if model performance improves even further.\n",
    "\n",
    "We'll use twelve estimators as before to have a fair comparison. There's no need to instantiate the decision tree as it is the base estimator by default.\n",
    "\n",
    "Instructions\n",
    "\n",
    "\n",
    "Build and fit an AdaBoostRegressor using 12 estimators. You do not have to specify a base estimator.\n",
    "Calculate the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Build and fit a tree-based AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(n_estimators=12, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     RMSE: 5.443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the most of AdaBoost\n",
    "\n",
    "As you have seen, for predicting movie revenue, AdaBoost gives the best results with decision trees as the base estimator.\n",
    "\n",
    "In this exercise, you'll specify some parameters to extract even more performance. In particular, you'll use a lower learning rate to have a smoother update of the hyperparameters. Therefore, the number of estimators should increase. Additionally, the following features have been added to the data: 'runtime', 'vote_average', and 'vote_count'.\n",
    "\n",
    "Instructions\n",
    "\n",
    "\n",
    "Build an AdaBoostRegressor using 100 estimators and a learning rate of 0.01.\n",
    "Fit reg_ada to the training set and calculate the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Build and fit an AdaBoost regressor with 100 estimators and a learning rate of 0.01\n",
    "reg_ada = AdaBoostRegressor(n_estimators=100, learning_rate=0.01, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE: {:.3f}'.format(rmse))\n",
    "\n",
    "# <script.py> output:\n",
    "#     RMSE: 5.150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with GBM\n",
    "\n",
    "Let's now use scikit-learn's GradientBoostingClassifier on the reviews dataset to predict the sentiment of a review given its text.\n",
    "\n",
    "We will not pass the raw text as input for the model. The following pre-processing has been done for you:\n",
    "\n",
    "Remove reviews with missing values.\n",
    "Select data from the top 5 apps.\n",
    "Select a random subsample of 500 reviews.\n",
    "Remove \"stop words\" from the reviews.\n",
    "Transform the reviews into a matrix, in which each feature represents the frequency of a word in a review.\n",
    "Do you want a deeper understanding of text mining? Then go check the course Introduction to Natural Language Processing in Python!\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build a GradientBoostingClassifier with 100 estimators and a learning rate of 0.1.\n",
    "Calculate the predictions on the test set.\n",
    "Compute the accuracy to evaluate the model.\n",
    "Calculate and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n",
      "[[147   3]\n",
      " [  2   8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Build and fit a Gradient Boosting classifier\n",
    "clf_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=500)\n",
    "clf_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance based on the accuracy\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy: {:.3f}'.format(acc))\n",
    "\n",
    "# Get and show the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy: 0.920\n",
    "#     [[29  0  5]\n",
    "#      [ 0  2  1]\n",
    "#      [ 2  0 61]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie revenue prediction with CatBoost\n",
    "\n",
    "Let's finish up this chapter on boosting by returning to the movies dataset! In this exercise, you'll build a CatBoostRegressor to predict the log-revenue. Remember that our best model so far is the AdaBoost model with a RMSE of 5.15.\n",
    "\n",
    "Will CatBoost beat AdaBoost? We'll try to use a similar set of parameters to have a fair comparison.\n",
    "\n",
    "Recall that these are the features we have used so far: 'budget', 'popularity', 'runtime', 'vote_average', and 'vote_count'. catboost has been imported for you as cb.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build and fit a CatBoostRegressor using 100 estimators, a learning rate of 0.1, and a max depth of 3.\n",
    "Calculate the predictions for the test set and print the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Build and fit a CatBoost regressor\n",
    "reg_cat = CatBoostRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=500)\n",
    "reg_cat.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = reg_cat.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print('RMSE (CatBoost): {:.3f}'.format(rmse_cat))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     0:\tlearn: 7.6907203\ttotal: 46.4ms\tremaining: 4.59s\n",
    "#     1:\tlearn: 7.3007402\ttotal: 46.9ms\tremaining: 2.3s\n",
    "#     2:\tlearn: 6.9617614\ttotal: 47.4ms\tremaining: 1.53s\n",
    "#     3:\tlearn: 6.6581011\ttotal: 47.9ms\tremaining: 1.15s\n",
    "#     4:\tlearn: 6.4042777\ttotal: 48.4ms\tremaining: 920ms\n",
    "#     5:\tlearn: 6.1917616\ttotal: 48.9ms\tremaining: 767ms\n",
    "#     6:\tlearn: 6.0184946\ttotal: 49.4ms\tremaining: 657ms\n",
    "#     7:\tlearn: 5.8619630\ttotal: 50ms\tremaining: 574ms\n",
    "#     8:\tlearn: 5.7238913\ttotal: 50.5ms\tremaining: 510ms\n",
    "#     9:\tlearn: 5.6222347\ttotal: 51ms\tremaining: 459ms\n",
    "#     10:\tlearn: 5.5264312\ttotal: 51.5ms\tremaining: 417ms\n",
    "#     11:\tlearn: 5.4410032\ttotal: 52ms\tremaining: 381ms\n",
    "#     12:\tlearn: 5.3740283\ttotal: 52.5ms\tremaining: 351ms\n",
    "#     13:\tlearn: 5.3179790\ttotal: 53ms\tremaining: 325ms\n",
    "#     14:\tlearn: 5.2642995\ttotal: 53.5ms\tremaining: 303ms\n",
    "#     15:\tlearn: 5.2250755\ttotal: 54ms\tremaining: 283ms\n",
    "#     16:\tlearn: 5.1923227\ttotal: 54.5ms\tremaining: 266ms\n",
    "#     17:\tlearn: 5.1634952\ttotal: 54.9ms\tremaining: 250ms\n",
    "#     18:\tlearn: 5.1402165\ttotal: 55.4ms\tremaining: 236ms\n",
    "#     19:\tlearn: 5.1142076\ttotal: 55.9ms\tremaining: 224ms\n",
    "#     20:\tlearn: 5.0938207\ttotal: 56.4ms\tremaining: 212ms\n",
    "#     21:\tlearn: 5.0786519\ttotal: 56.9ms\tremaining: 202ms\n",
    "#     22:\tlearn: 5.0627297\ttotal: 57.4ms\tremaining: 192ms\n",
    "#     23:\tlearn: 5.0512657\ttotal: 57.9ms\tremaining: 183ms\n",
    "#     24:\tlearn: 5.0409775\ttotal: 58.4ms\tremaining: 175ms\n",
    "#     25:\tlearn: 5.0312806\ttotal: 58.9ms\tremaining: 168ms\n",
    "#     26:\tlearn: 5.0219800\ttotal: 59.4ms\tremaining: 160ms\n",
    "#     27:\tlearn: 5.0136052\ttotal: 59.8ms\tremaining: 154ms\n",
    "#     28:\tlearn: 5.0066007\ttotal: 60.3ms\tremaining: 148ms\n",
    "#     29:\tlearn: 5.0002061\ttotal: 60.8ms\tremaining: 142ms\n",
    "#     30:\tlearn: 4.9938356\ttotal: 61.2ms\tremaining: 136ms\n",
    "#     31:\tlearn: 4.9891673\ttotal: 61.7ms\tremaining: 131ms\n",
    "#     32:\tlearn: 4.9849912\ttotal: 62.2ms\tremaining: 126ms\n",
    "#     33:\tlearn: 4.9818629\ttotal: 62.6ms\tremaining: 122ms\n",
    "#     34:\tlearn: 4.9769315\ttotal: 63.1ms\tremaining: 117ms\n",
    "#     35:\tlearn: 4.9734819\ttotal: 63.6ms\tremaining: 113ms\n",
    "#     36:\tlearn: 4.9706818\ttotal: 64ms\tremaining: 109ms\n",
    "#     37:\tlearn: 4.9667811\ttotal: 64.6ms\tremaining: 105ms\n",
    "#     38:\tlearn: 4.9635038\ttotal: 65ms\tremaining: 102ms\n",
    "#     39:\tlearn: 4.9614684\ttotal: 65.5ms\tremaining: 98.2ms\n",
    "#     40:\tlearn: 4.9598851\ttotal: 65.9ms\tremaining: 94.9ms\n",
    "#     41:\tlearn: 4.9563059\ttotal: 66.4ms\tremaining: 91.7ms\n",
    "#     42:\tlearn: 4.9546486\ttotal: 66.8ms\tremaining: 88.6ms\n",
    "#     43:\tlearn: 4.9534317\ttotal: 67.3ms\tremaining: 85.6ms\n",
    "#     44:\tlearn: 4.9527290\ttotal: 67.7ms\tremaining: 82.8ms\n",
    "#     45:\tlearn: 4.9515974\ttotal: 68.2ms\tremaining: 80ms\n",
    "#     46:\tlearn: 4.9491063\ttotal: 68.7ms\tremaining: 77.4ms\n",
    "#     47:\tlearn: 4.9473837\ttotal: 69.1ms\tremaining: 74.9ms\n",
    "#     48:\tlearn: 4.9436646\ttotal: 69.6ms\tremaining: 72.4ms\n",
    "#     49:\tlearn: 4.9428917\ttotal: 70ms\tremaining: 70ms\n",
    "#     50:\tlearn: 4.9416607\ttotal: 70.5ms\tremaining: 67.7ms\n",
    "#     51:\tlearn: 4.9401623\ttotal: 70.9ms\tremaining: 65.5ms\n",
    "#     52:\tlearn: 4.9393256\ttotal: 71.3ms\tremaining: 63.3ms\n",
    "#     53:\tlearn: 4.9387984\ttotal: 71.8ms\tremaining: 61.1ms\n",
    "#     54:\tlearn: 4.9367738\ttotal: 72.3ms\tremaining: 59.1ms\n",
    "#     55:\tlearn: 4.9340331\ttotal: 72.7ms\tremaining: 57.1ms\n",
    "#     56:\tlearn: 4.9315110\ttotal: 73.2ms\tremaining: 55.2ms\n",
    "#     57:\tlearn: 4.9290670\ttotal: 73.7ms\tremaining: 53.3ms\n",
    "#     58:\tlearn: 4.9284069\ttotal: 74.1ms\tremaining: 51.5ms\n",
    "#     59:\tlearn: 4.9276722\ttotal: 74.5ms\tremaining: 49.7ms\n",
    "#     60:\tlearn: 4.9272245\ttotal: 74.9ms\tremaining: 47.9ms\n",
    "#     61:\tlearn: 4.9264551\ttotal: 75.4ms\tremaining: 46.2ms\n",
    "#     62:\tlearn: 4.9258886\ttotal: 75.9ms\tremaining: 44.6ms\n",
    "#     63:\tlearn: 4.9243086\ttotal: 76.3ms\tremaining: 42.9ms\n",
    "#     64:\tlearn: 4.9225200\ttotal: 76.8ms\tremaining: 41.3ms\n",
    "#     65:\tlearn: 4.9199979\ttotal: 77.2ms\tremaining: 39.8ms\n",
    "#     66:\tlearn: 4.9187091\ttotal: 77.7ms\tremaining: 38.3ms\n",
    "#     67:\tlearn: 4.9180491\ttotal: 78.2ms\tremaining: 36.8ms\n",
    "#     68:\tlearn: 4.9154203\ttotal: 78.6ms\tremaining: 35.3ms\n",
    "#     69:\tlearn: 4.9144691\ttotal: 79.1ms\tremaining: 33.9ms\n",
    "#     70:\tlearn: 4.9141659\ttotal: 79.5ms\tremaining: 32.5ms\n",
    "#     71:\tlearn: 4.9136370\ttotal: 79.9ms\tremaining: 31.1ms\n",
    "#     72:\tlearn: 4.9120578\ttotal: 80.4ms\tremaining: 29.7ms\n",
    "#     73:\tlearn: 4.9110179\ttotal: 80.8ms\tremaining: 28.4ms\n",
    "#     74:\tlearn: 4.9097900\ttotal: 81.2ms\tremaining: 27.1ms\n",
    "#     75:\tlearn: 4.9088271\ttotal: 81.7ms\tremaining: 25.8ms\n",
    "#     76:\tlearn: 4.9082645\ttotal: 82.1ms\tremaining: 24.5ms\n",
    "#     77:\tlearn: 4.9073784\ttotal: 82.5ms\tremaining: 23.3ms\n",
    "#     78:\tlearn: 4.9054126\ttotal: 83ms\tremaining: 22.1ms\n",
    "#     79:\tlearn: 4.9041859\ttotal: 83.4ms\tremaining: 20.9ms\n",
    "#     80:\tlearn: 4.9038452\ttotal: 83.8ms\tremaining: 19.7ms\n",
    "#     81:\tlearn: 4.9033286\ttotal: 84.2ms\tremaining: 18.5ms\n",
    "#     82:\tlearn: 4.9024389\ttotal: 84.7ms\tremaining: 17.3ms\n",
    "#     83:\tlearn: 4.9016633\ttotal: 85.1ms\tremaining: 16.2ms\n",
    "#     84:\tlearn: 4.8993908\ttotal: 85.5ms\tremaining: 15.1ms\n",
    "#     85:\tlearn: 4.8988986\ttotal: 85.9ms\tremaining: 14ms\n",
    "#     86:\tlearn: 4.8976443\ttotal: 86.4ms\tremaining: 12.9ms\n",
    "#     87:\tlearn: 4.8958643\ttotal: 86.8ms\tremaining: 11.8ms\n",
    "#     88:\tlearn: 4.8950438\ttotal: 87.2ms\tremaining: 10.8ms\n",
    "#     89:\tlearn: 4.8927841\ttotal: 87.7ms\tremaining: 9.74ms\n",
    "#     90:\tlearn: 4.8899605\ttotal: 88.1ms\tremaining: 8.72ms\n",
    "#     91:\tlearn: 4.8890892\ttotal: 88.6ms\tremaining: 7.7ms\n",
    "#     92:\tlearn: 4.8869003\ttotal: 89ms\tremaining: 6.7ms\n",
    "#     93:\tlearn: 4.8858148\ttotal: 89.4ms\tremaining: 5.71ms\n",
    "#     94:\tlearn: 4.8826237\ttotal: 89.8ms\tremaining: 4.73ms\n",
    "#     95:\tlearn: 4.8801529\ttotal: 90.3ms\tremaining: 3.76ms\n",
    "#     96:\tlearn: 4.8785101\ttotal: 90.7ms\tremaining: 2.81ms\n",
    "#     97:\tlearn: 4.8761696\ttotal: 91.2ms\tremaining: 1.86ms\n",
    "#     98:\tlearn: 4.8742715\ttotal: 91.6ms\tremaining: 925us\n",
    "#     99:\tlearn: 4.8737823\ttotal: 92ms\tremaining: 0us\n",
    "#     RMSE (CatBoost): 5.115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting contest: Light vs Extreme\n",
    "\n",
    "While the performance of the CatBoost model is relatively good, let's try two other flavors of boosting and see which performs better: the \"Light\" or the \"Extreme\" approach.\n",
    "\n",
    "CatBoost is highly recommended when there are categorical features. In this case, all features are numeric, therefore one of the other approaches might produce better results.\n",
    "\n",
    "As we are building regressors, we'll use an additional parameter, objective, which specifies the learning function to be used. To apply a squared error, we'll set objective to 'reg:squarederror' for XGBoost and 'mean_squared_error' for LightGBM.\n",
    "\n",
    "In addition, we'll specify the parameter n_jobs for XGBoost to improve its computation runtime.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build an XGBRegressor using the parameters: max_depth = 3, learning_rate = 0.1, n_estimators = 100, and n_jobs=2.\n",
    "Build an LGBMRegressor using the parameters: max_depth = 3, learning_rate = 0.1, and n_estimators = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Build and fit an XGBoost regressor\n",
    "reg_xgb = XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, n_jobs=2, objective='reg:squarederror', random_state=500)\n",
    "reg_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Build and fit a LightGBM regressor\n",
    "reg_lgb = lgb.LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, objective='mean_squared_error', seed=500)\n",
    "reg_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions and evaluate both regressors\n",
    "pred_xgb = reg_xgb.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "pred_lgb = reg_lgb.predict(X_test)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "\n",
    "print('Extreme: {:.3f}, Light: {:.3f}'.format(rmse_xgb, rmse_lgb))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
    "#     You can set `force_col_wise=true` to remove the overhead.\n",
    "#     [LightGBM] [Info] Total Bins 904\n",
    "#     [LightGBM] [Info] Number of data points in the train set: 3842, number of used features: 5\n",
    "#     [LightGBM] [Info] Start training from score 12.266868\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "#     Extreme: 5.122, Light: 5.142\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 4 - Stacking**\n",
    "\n",
    "Get ready to see how things stack up! In this final chapter you'll learn about the stacking ensemble method. You'll learn how to implement it using scikit-learn as well as with the mlxtend library! You'll apply stacking to predict the edibility of North American mushrooms, and revisit the ratings of Google apps with this more advanced approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting mushroom edibility\n",
    "\n",
    "Now that you have explored the data, it's time to build a first model to predict mushroom edibility.\n",
    "\n",
    "The dataset is available to you as mushrooms. As both the features and the target are categorical, these have been transformed into \"dummy\" binary variables for you.\n",
    "\n",
    "Let's begin with Naive Bayes (using scikit-learn's GaussianNB) and see how this algorithm performs on this problem.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate a GaussianNB classifier called clf_nb.\n",
    "Fit clf_nb to the training data X_train and y_train.\n",
    "Calculate the predictions on the test set. These predictions will be used to evaluate the performance using the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Instantiate a Naive Bayes classifier\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Fit the model to the training set\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_nb.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the accuracy score\n",
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors for mushrooms\n",
    "\n",
    "The Gaussian Naive Bayes classifier did a really good job for being an initial model. Let's now build a new model to compare it against the Naive Bayes.\n",
    "\n",
    "In this case, the algorithm to use is a 5-nearest neighbors classifier. As the dummy features create a high-dimensional dataset, use the Ball Tree algorithm to make the model faster. Let's see how this model performs!\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build a KNeighborsClassifier with 5 neighbors and algorithm = 'ball_tree' (to expedite the processing).\n",
    "Fit the model to the training data.\n",
    "Evaluate the performance on the test set using the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate a 5-nearest neighbors classifier with 'ball_tree' algorithm\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "# Fit the model to the training set\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred = clf_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the accuracy score\n",
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying stacking to predict app ratings\n",
    "\n",
    "In this exercise you'll start building your first Stacking ensemble. The dataset you'll use is the first one we used in Chapter 1. If you recall, the objective is to predict the rating of each app (from 1 to 5). The input features we use are: Reviews, Size, Installs, Type, Price, and Content Rating.\n",
    "\n",
    "We already did step 1: prepare the dataset. It is available to you as apps. We cleaned the required features and replaced missing values with zeros.\n",
    "\n",
    "Now, you'll work on step 2: build the first-layer estimators.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Build and fit a decision tree classifier with: min_samples_leaf: 3 and min_samples_split: 9.\n",
    "Build and fit a 5-nearest neighbors classifier using: algorithm: 'ball_tree' (to expedite the processing).\n",
    "Evaluate the performance of each estimator using the accuracy score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: 0.9688\n",
      "5-Nearest Neighbors: 0.9688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build and fit a Decision Tree classifier\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "# Build and fit a 5-nearest neighbors classifier using the 'Ball-Tree' algorithm\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance using the accuracy score\n",
    "print('Decision Tree: {:0.4f}'.format(accuracy_score(y_test, clf_dt.predict(X_test))))\n",
    "print('5-Nearest Neighbors: {:0.4f}'.format(accuracy_score(y_test, clf_knn.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the stacking classifier\n",
    "\n",
    "Now you'll work on the next two steps.\n",
    "\n",
    "Step 3: Append the predictions to the dataset: this is internally handled by the StackingClassifier class, but we'll do our part by preparing the list of first-level classifiers, which you built in the previous exercise. These are available as: clf_dt and clf_knn.\n",
    "\n",
    "Step 4: Build the second-layer meta estimator: for this purpose you'll use the default LogisticRegression. This will take as input features the individual predictions from the base estimators.\n",
    "\n",
    "With both levels of estimators ready you can build the stacking classifier.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Prepare the list of tuples with the first-layer classifiers: clf_dt and clf_knn (specifying the appropriate labels and order).\n",
    "Instantiate the second-layer meta estimator: a LogisticRegression.\n",
    "Build the stacking classifier passing: the list of tuples, the meta classifier, with stack_method='predict_proba' (to use class probabilities), and passthrough = False (to only use predictions as features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the list of tuples with the first-layer classifiers\n",
    "classifiers = [\n",
    "\t('clf_dt', clf_dt),\n",
    "    ('clf_knn', clf_knn)\n",
    "]\n",
    "\n",
    "# Instantiate the second-layer meta estimator\n",
    "clf_meta = LogisticRegression()\n",
    "\n",
    "# Build the stacking classifier\n",
    "clf_stack = StackingClassifier(\n",
    "   estimators=classifiers,\n",
    "   final_estimator=clf_meta,\n",
    "   stack_method='predict_proba',\n",
    "   passthrough=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked predictions for app ratings\n",
    "\n",
    "Once the stacking estimator is built you can fit it to the training set. Then, it will be ready for step 5: use the stacked ensemble for predictions.\n",
    "\n",
    "The stacking classifier is available to you as clf_stack.\n",
    "\n",
    "Let's obtain the final predictions and see if there is any improvement in performance thanks to stacking.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Fit the stacking classifier on the training set.\n",
    "Calculate the final predictions from the stacking estimator on the test set.\n",
    "Evaluate the performance on the test set using the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the stacking classifier to the training set\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Obtain the final predictions from the stacking classifier\n",
    "pred_stack = clf_stack.predict(X_test)\n",
    "\n",
    "# Evaluate the new performance on the test set\n",
    "print('Accuracy: {:0.4f}'.format(accuracy_score(y_test, pred_stack)))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy: 0.6424"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first attempt with mlxtend\n",
    "\n",
    "It's time to start working with mlxtend! You'll continue using the app ratings dataset. As you have already built a stacked ensemble model using scikit-learn, you have a basis to compare with the model you'll now build with mlxtend.\n",
    "\n",
    "The dataset is loaded and available to you as apps.\n",
    "\n",
    "Let's see if mlxtend can build a model as good as or better than the scikit-learn ensemble classifier.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate a decision tree classifier with min_samples_leaf = 3 and min_samples_split = 9.\n",
    "Instantiate a 5-nearest neighbors classifier using the 'ball_tree' algorithm.\n",
    "Build a StackingClassifier passing: the list of classifiers, the meta classifier, use_probas=True (to use probabilities), and use_features_in_secondary = False (to only use the individual predictions).\n",
    "Evaluate the performance by computing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the first-layer classifiers\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "# Instantiate the second-layer meta classifier\n",
    "clf_meta = LogisticRegression()\n",
    "\n",
    "# Build the Stacking classifier\n",
    "clf_stack = StackingClassifier(classifiers=[clf_dt, clf_knn], meta_classifier=clf_meta, use_probas=True, use_features_in_secondary=False)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the Stacking classifier\n",
    "pred_stack = clf_stack.predict(X_test)\n",
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_stack)))\n",
    "\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy: 0.6050\n",
    "\n",
    "# <script.py> output:\n",
    "#     Accuracy: 0.6050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to regression with stacking\n",
    "\n",
    "In Chapter 1, we treated the app ratings as a regression problem, predicting the rating on the interval from 1 to 5. So far in this chapter, we have dealt with it as a classification problem, rounding the rating to the nearest integer. To practice using the StackingRegressor, we'll go back to the regression approach. As usual, the input features have been standardized for you with a StandardScaler().\n",
    "\n",
    "The MAE (mean absolute error) is the evaluation metric. In Chapter 1, the MAE was around 0.61. Let's see if the stacking ensemble method can reduce that error.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate a decision tree regressor with: min_samples_leaf = 11 and min_samples_split = 33.\n",
    "Instantiate the default linear regression.\n",
    "Instantiate a Ridge regression model with random_state = 500.\n",
    "Build and fit a StackingRegressor, passing the regressors and the meta_regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeiso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\yeiso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\yeiso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but Ridge was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Instantiate the 1st-layer regressors\n",
    "reg_dt = DecisionTreeRegressor(min_samples_leaf=11, min_samples_split=33, random_state=500)\n",
    "reg_lr = LinearRegression()\n",
    "reg_ridge = Ridge(random_state=500)\n",
    "\n",
    "# Instantiate the 2nd-layer regressor\n",
    "reg_meta = LinearRegression()\n",
    "\n",
    "# Build the Stacking regressor\n",
    "reg_stack = StackingRegressor(regressors=[reg_dt, reg_lr, reg_ridge], \n",
    "                               meta_regressor=reg_meta)\n",
    "\n",
    "# Fit the stacking regressor to the training set\n",
    "reg_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance on the test set using the MAE metric\n",
    "pred = reg_stack.predict(X_test)\n",
    "print('MAE: {:.3f}'.format(mean_absolute_error(y_test, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushrooms: a matter of life or death\n",
    "\n",
    "Let's conclude the course by revisiting the mushroom edibility problem. You'll try the stacking classifier to see if the score can be improved. As stacking uses a meta-estimator (second layer classifier) which attempts to correct predictions from the first layer, some of the misclassified instances could be corrected. This is a very important problem, as the edibility of a mushroom is a matter of life or death.\n",
    "\n",
    "The dataset has been loaded and split into train and test sets. Do you think stacking can help to predict the edibility of a mushroom with greater confidence?\n",
    "\n",
    "Instructions\n",
    "\n",
    "Instantiate the first-layer estimators: a 5-nearest neighbors using the ball tree algorithm, a decision tree classifier with parameters min_samples_leaf = 5 and min_samples_split = 15, and a Gaussian Naive Bayes classifier.\n",
    "Build and fit a stacking classifier, using the parameters classifiers - a list containing the first-layer classifiers - and meta_classifier - the default logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Create the first-layer models\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "clf_dt = DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=15, random_state=500)\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Create the second-layer model (meta-model)\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# Create and fit the stacked model\n",
    "clf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb], meta_classifier=clf_lr)\n",
    "clf_stack.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacked model’s performance\n",
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, clf_stack.predict(X_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
