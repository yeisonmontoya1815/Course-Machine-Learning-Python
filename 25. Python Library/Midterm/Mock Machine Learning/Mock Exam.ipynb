{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dc2cd0",
   "metadata": {},
   "source": [
    "## Mock Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b0042",
   "metadata": {},
   "source": [
    "### Part 1: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f8838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('winequality.csv')\n",
    "df=pd.DataFrame(data)\n",
    "#Remove all rows that contain NaN values in the original Dataframe\n",
    "df.dropna(how='all',inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "##df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a67a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "##df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1ebc8",
   "metadata": {},
   "source": [
    "### Part 2: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d9131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'fixed acidity' : 'fixed_acidity'},inplace=True)\n",
    "df.rename(columns={'volatile acidity' : 'volatile_acidity'},inplace=True)\n",
    "df.rename(columns={'citric acid' : 'citric_acid'},inplace=True)\n",
    "df.rename(columns={'residual sugar' : 'residual_sugar'},inplace=True)\n",
    "df.rename(columns={'free sulfur dioxide' : 'free_sulfur_dioxide'},inplace=True)\n",
    "df.rename(columns={'total sulfur dioxide' : 'total_sulfur_dioxide'},inplace=True)\n",
    "##df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdaa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "##df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968a9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.scatter(df.sulphates, df.quality)\n",
    "##plt.title(\"Quality vs Sulphates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cfbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.scatter(df.pH, df.quality)\n",
    "##plt.title(\"Quality vs pH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ba5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.scatter(df.citric_acid , df.quality)\n",
    "##plt.title(\"Quality vs Citric Acid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1e5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.scatter(df.alcohol, df.quality)\n",
    "##plt.title(\"Quality vs Alcohol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b39a0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(df.volatile_acidity, df.quality)\n",
    "#plt.title(\"Quality vs Volatile Acidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f0a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.scatter(df.chlorides, df.quality)\n",
    "##plt.title(\"Quality vs Chlorides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16bccc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sns.pairplot(df, height=2.5)\n",
    "##plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c8c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cm = np.corrcoef(df.values.T)\n",
    "##sns.set(font_scale=0.8)\n",
    "##hm= sns.heatmap(cm,\n",
    "##                  cbar=True,\n",
    "##                  annot=True,\n",
    "##                  square=True,\n",
    "##                  fmt ='.2f',\n",
    "##                  annot_kws={'size':10},\n",
    "##                  yticklabels=df.columns,\n",
    "##                  xticklabels=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "933ae061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4627b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['fixed_acidity','residual_sugar','free_sulfur_dioxide', \n",
    "         'pH','chlorides','total_sulfur_dioxide','density'],axis=1,inplace=True)\n",
    "#df.drop(df.loc[df[\"quality\"]>7.5].index, inplace=True)\n",
    "#df.drop(df.loc[df[\"alcohol\"]>12].index, inplace=True)\n",
    "#df.drop(df.loc[df[\"total_sulfur_dioxide\"]>80].index, inplace=True)\n",
    "#df.drop(df.loc[df[\"volatile_acidity\"]>1.58].index, inplace=True)\n",
    "##df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9da198",
   "metadata": {},
   "source": [
    "### Analysis ....\n",
    "In this dataset has been eliminated the columns with lower correlation with quality and areas close to 0, so had a low association, all with value in the range 0.2 and -0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f493ab",
   "metadata": {},
   "source": [
    "### Part 3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16019cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = []\n",
    "r2_list_train = []\n",
    "r2_list_test = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecda3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "response = df['quality']\n",
    "features = df.drop('quality', axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, \n",
    "                                                   response,\n",
    "                                                   test_size =0.1, \n",
    "                                                   random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d49053bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()\n",
    "#Training model\n",
    "model.fit(X_train, Y_train )\n",
    "#Making predictions \n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13ba5e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 in the train set is  0.3426857567444026\n",
      "R2 in the test set is  0.2948268456209844\n",
      "RMSE in the test set is mse 0.6004304498602445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "r2_train = model.score(X_train, Y_train)\n",
    "print(\"R2 in the train set is \", r2_train)\n",
    "r2= model.score(X_test, Y_test)\n",
    "print(\"R2 in the test set is \", r2)\n",
    "\n",
    "rmse= mean_squared_error(Y_test, predictions) ** 0.5\n",
    "print(\"RMSE in the test set is mse\",rmse )\n",
    "\n",
    "method.append('Linear Regression')\n",
    "r2_list_train.append(r2_train)\n",
    "r2_list_test.append(r2)\n",
    "rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce2a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##actual_value = Y_test\n",
    "##plt.scatter(predictions, actual_value, alpha=0.6)\n",
    "##plt.xlabel(\"Predicted value\")\n",
    "##plt.ylabel(\"Actual value\")\n",
    "##plt.title(\"Linear Regression Model\")\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8ea09",
   "metadata": {},
   "source": [
    "### Using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c27ab68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Split Data set \n",
    "response = df['quality']\n",
    "features = df.drop('quality', axis=1)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(features, \\\n",
    "                                                response, \\\n",
    "                                                test_size=0.1, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeec14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a list of aplhas\n",
    "#alphas = 10**np.linspace(6, -2, 20)\n",
    "alphas = [100, 10, 1, 0.1, 1e-2, 1e-4, 1e-6, 1e-8]\n",
    "##alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfba8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha = a)\n",
    "    lasso.fit(features,response)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "##np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3ef5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.plot(alphas, coefs)\n",
    "##ax = plt.gca()\n",
    "##ax.set_xscale('log')\n",
    "##plt.axis('tight')\n",
    "##plt.xlabel('alpha')\n",
    "##plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41b8423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Findinfg the best alpha\n",
    "rmse_test_list = []\n",
    "r2_test_list = []\n",
    "r2_train_list = []\n",
    "best_r2_test = 0\n",
    "best_alpha = 0\n",
    "\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha = a, max_iter=1000)\n",
    "    lasso.fit(xtrain, ytrain)\n",
    "    pred = lasso.predict(xtest)\n",
    "    \n",
    "    r2_train = lasso.score(xtrain,ytrain)\n",
    "    r2_train_list.append(r2_train)\n",
    "    \n",
    "    r2_test = lasso.score(xtest,ytest)\n",
    "    r2_test_list.append(r2_test)\n",
    "    \n",
    "    rmse_test = mean_squared_error(ytest, pred) ** 0.5\n",
    "    rmse_test_list.append(rmse_test)\n",
    "    \n",
    "    if r2_test > best_r2_test:\n",
    "        best_r2_test = r2_test\n",
    "        best_alpha = a\n",
    "    \n",
    "lasso_result = np.vstack((alphas, \\\n",
    "                         r2_train_list, \\\n",
    "                         r2_test_list, \\\n",
    "                         rmse_test_list)).T\n",
    "lasso_df = pd.DataFrame(lasso_result, \\\n",
    "                            columns=['Alpha', 'R2 Train', 'R2 Test', 'RMSE'])\n",
    "\n",
    "##print(lasso_df)\n",
    "##print(best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09adbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=best_alpha)\n",
    "lasso.fit(xtrain, ytrain)\n",
    "pred = lasso.predict(xtest)\n",
    "\n",
    "r2_test =lasso.score(xtest,ytest)\n",
    "r2_train = lasso.score(xtrain,ytrain)\n",
    "rmse= mean_squared_error(pred,ytest) ** 0.5\n",
    "##print(\"R^2 train:\", r2_train)\n",
    "##print(\"R^2 test:\", r2_test)\n",
    "##print(\"RMSE test:\", rmse)\n",
    "##print()\n",
    "##print(pd.Series(lasso.coef_, index = features.columns))\n",
    "\n",
    "method.append('Lasso')\n",
    "r2_list_test.append(r2_test)\n",
    "r2_list_train.append(r2_train)\n",
    "rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8107b62",
   "metadata": {},
   "source": [
    "### 4 Linear Regression with Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c957577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "zscore = ss.fit_transform(features)\n",
    "feature_ss = pd.DataFrame(zscore, \\\n",
    "                          index=features.index, columns=features.columns)\n",
    "feature_ss = feature_ss.reset_index(drop=True)\n",
    "##print(feature_ss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3c54c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "X_train_ss, X_test_ss, Y_train_ss, Y_test_ss = train_test_split( \\\n",
    "                                                               feature_ss, \\\n",
    "                                                                response, \\\n",
    "                                                                test_size=0.1, \\\n",
    "                                                                random_state=42 )\n",
    "model_ss = LinearRegression()\n",
    "model_ss.fit(X_train_ss, Y_train_ss)\n",
    "pred_ss = model_ss.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ddab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2\n",
    "r2_train_ss = model_ss.score(X_train_ss, Y_train_ss)\n",
    "##print(\"R^2 in Training Set:\", r2_train_ss)\n",
    "r2_test_ss = model_ss.score(X_test_ss, Y_test_ss)\n",
    "##print(\"R^2 in Test Set:\", r2_test_ss)\n",
    "# rmse\n",
    "rmse_ss = mean_squared_error(Y_test_ss, pred_ss) ** 0.5\n",
    "##print('RMSE:', rmse_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f95f5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the result\n",
    "method.append('Standard Scaler')\n",
    "r2_list_test.append(r2_test_ss)\n",
    "r2_list_train.append(r2_train_ss)\n",
    "rmse_list.append(rmse_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45a55e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   volatile_acidity  citric_acid  sulphates   alcohol\n",
      "0          0.397260         0.00   0.137725  0.153846\n",
      "1          0.520548         0.00   0.209581  0.215385\n",
      "2          0.438356         0.04   0.191617  0.215385\n",
      "3          0.109589         0.56   0.149701  0.215385\n",
      "4          0.369863         0.00   0.137725  0.153846\n"
     ]
    }
   ],
   "source": [
    "# minmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "minmax = mm.fit_transform(features)\n",
    "\n",
    "feature_mm = pd.DataFrame(minmax, \\\n",
    "                          index=features.index, \n",
    "                          columns=features.columns)\n",
    "feature_mm = feature_mm.reset_index(drop=True)\n",
    "print(feature_mm.head())\n",
    "\n",
    "X_train_mm, X_test_mm, Y_train_mm, Y_test_mm = train_test_split(feature_mm, \\\n",
    "                                                               response, \\\n",
    "                                                               test_size=0.1,\\\n",
    "                                                               random_state=42)\n",
    "model_mm = LinearRegression()\n",
    "model_mm.fit(X_train_mm, Y_train_mm)\n",
    "\n",
    "pred_mm = model_mm.predict(X_test_mm)\n",
    "\n",
    "r2_train_mm = model_mm.score(X_train_mm, Y_train_mm)\n",
    "##print(\"R^2 in Training Set:\", r2_train_mm)\n",
    "\n",
    "r2_test_mm = model_mm.score(X_test_mm, Y_test_mm)\n",
    "##print(\"R^2 in Test Set:\", r2_test_mm)\n",
    "\n",
    "rmse_mm = mean_squared_error(Y_test_mm, pred_mm) ** 0.5\n",
    "##print(\"RMSE:\", rmse_mm)\n",
    "\n",
    "method.append('MinMax Scaler')\n",
    "r2_list_train.append(r2_train_mm)\n",
    "r2_list_test.append(r2_test_mm)\n",
    "rmse_list.append(rmse_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4a4bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##actual_value = Y_test_mm\n",
    "##plt.scatter(pred_mm, actual_value, alpha=0.6)\n",
    "##plt.xlabel(\"Predicted value\")\n",
    "##plt.ylabel(\"Actual value\")\n",
    "##plt.title(\"Linear with Scaler MinMax\")\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977dab6",
   "metadata": {},
   "source": [
    "## Analysis ...\n",
    "Create a cell (a Markdown cell) in the Jupyter notebook and write down your observation and conclude if scaling is necessary.\n",
    "\n",
    "In this case, it's observed that values for Linear Regression with Scaling and Simple Linear Regression are pretty close, almost similar. So, it's unnecessary to apply scaling over this dataset. So, Scaling does not cause benefits over this dataset. Furthermore, the dimensions of the numbers are low and are not in accordance with this type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3ea88",
   "metadata": {},
   "source": [
    "### Part 5: Linear Regression with Ridge using Scaled Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b37145cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "zscore = ss.fit_transform(features)\n",
    "feature_ss = pd.DataFrame(zscore, \\\n",
    "                          index=features.index, columns=features.columns)\n",
    "feature_ss = feature_ss.reset_index(drop=True)\n",
    "##print(feature_ss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "353f6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "X_train_ss, X_test_ss, Y_train_ss, Y_test_ss = train_test_split( \\\n",
    "                                                               feature_ss, \\\n",
    "                                                                response, \\\n",
    "                                                                test_size=0.1, \\\n",
    "                                                                random_state=42 )\n",
    "model_ss = LinearRegression()\n",
    "model_ss.fit(X_train_ss, Y_train_ss)\n",
    "pred_ss = model_ss.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567a0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a list of aplhas\n",
    "##alphas = 10 ** np.linspace(6, -2, 20)\n",
    "alphas = [100, 10, 1, 0.1, 1e-2, 1e-4, 1e-6, 1e-8]\n",
    "##alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "615318af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(features,response)\n",
    "    coefs.append(ridge.coef_)\n",
    "##np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3181e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.plot(alphas, coefs)\n",
    "##ax = plt.gca()\n",
    "##ax.set_xscale('log')\n",
    "##plt.axis('tight')\n",
    "##plt.xlabel('alpha')\n",
    "##plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1f51203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Findinfg the best alpha\n",
    "\n",
    "rmse_test_list = []\n",
    "r2_test_list = []\n",
    "r2_train_list = []\n",
    "\n",
    "best_r2_test = 0\n",
    "best_alpha = 0\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha = a, max_iter=1000)\n",
    "    ridge.fit(X_train_ss, Y_train_ss)\n",
    "    pred = ridge.predict(X_test_ss)\n",
    "    \n",
    "    r2_train = ridge.score(X_train_ss, Y_train_ss)\n",
    "    r2_train_list.append(r2_train)\n",
    "    \n",
    "    r2_test = ridge.score(X_test_ss, Y_test_ss)\n",
    "    r2_test_list.append(r2_test)\n",
    "    \n",
    "    rmse_test = mean_squared_error(Y_test_ss, pred) ** 0.5\n",
    "    rmse_test_list.append(rmse_test)\n",
    "    \n",
    "    if r2_test > best_r2_test:\n",
    "        best_r2_test = r2_test\n",
    "        best_alpha = a\n",
    "    \n",
    "ridge_result = np.vstack((alphas, \\\n",
    "                         r2_train_list, \\\n",
    "                         r2_test_list, \\\n",
    "                         rmse_test_list)).T\n",
    "ridge_df = pd.DataFrame(ridge_result, \\\n",
    "                        columns=['Alpha', 'R2 Train', 'R2 Test', 'RMSE'])\n",
    "\n",
    "##print(ridge_df)\n",
    "##print(best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bff719ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ridge correction to the model considering the  previous best alpha founded\n",
    "ridge = Ridge(alpha=best_alpha)\n",
    "ridge.fit(X_train_ss, Y_train_ss)\n",
    "pred = ridge.predict(X_test_ss)\n",
    "\n",
    "r2_test =ridge.score(X_test_ss, Y_test_ss)\n",
    "r2_train = ridge.score(X_train_ss, Y_train_ss)\n",
    "rmse= mean_squared_error(pred,Y_test_ss) ** 0.5\n",
    "##print(\"R^2 train:\", r2_train)\n",
    "##print(\"R^2 test:\", r2_test)\n",
    "##print(\"RMSE test:\", rmse)\n",
    "##print()\n",
    "##print(pd.Series(ridge.coef_, index = features.columns))\n",
    "\n",
    "method.append('Ridge using Scaler')\n",
    "r2_list_train.append(r2_train)\n",
    "r2_list_test.append(r2_test)\n",
    "rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ec75a",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d963a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Split datasets\n",
    "response = df['quality'] \n",
    "features = df.drop('quality', axis=1) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, \n",
    "                                                   response,\n",
    "                                                   test_size =0.1,\n",
    "                                                   random_state= 1024)\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98879c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#Finding best degree\n",
    "best_degree = 0\n",
    "best_mse = 0\n",
    "best_model = None\n",
    "best_r2=0\n",
    "\n",
    "for degree in range(1, 5):\n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=degree) \n",
    "    \n",
    "    X_train_poly = poly_features.fit_transform(X_train) \n",
    "    X_test_poly = poly_features.transform(X_test) \n",
    "    model.fit(X_train_poly, Y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    \n",
    "    r2= model.score(X_test_poly, Y_test)\n",
    "    rmse_test = mean_squared_error(Y_test, y_pred) ** 0.5\n",
    "\n",
    "    if r2 > best_r2:\n",
    "        best_r2= r2\n",
    "        best_degree = degree\n",
    "        best_mse = rmse_test\n",
    "        \n",
    "##print(f\"Best degree: {best_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ab2e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_reg = PolynomialFeatures(degree = best_degree)\n",
    "\n",
    "X_train_poly = poli_reg.fit_transform(X_train)\n",
    "X_test_poly = poli_reg.fit_transform(X_test)\n",
    "\n",
    "model.fit(X_train_poly, Y_train )\n",
    "prediction = model.predict(X_test_poly)\n",
    "\n",
    "r2_train= model.score(X_train_poly, Y_train)\n",
    "r2_test= model.score(X_test_poly, Y_test)\n",
    "rmse_test = mean_squared_error(Y_test, prediction) ** 0.5\n",
    "##print(\"R^2 train:\", r2_train)\n",
    "##print(\"R^2 test:\", r2_test)\n",
    "##print(\"RMSE test:\", rmse_test)\n",
    "\n",
    "method.append('Polynomial')\n",
    "r2_list_train.append(r2_train)\n",
    "r2_list_test.append(r2_test)\n",
    "rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14710f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##actual_value = Y_test\n",
    "##plt.scatter(predictions, actual_value, alpha=0.6)\n",
    "##plt.xlabel(\"Predicted value\")\n",
    "##plt.ylabel(\"Actual value\")\n",
    "##plt.title(\"Polynomial Regression Model\")\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0b3415c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>R2 Training</th>\n",
       "      <th>R2 Test</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.3426857567444026</td>\n",
       "      <td>0.2948268456209844</td>\n",
       "      <td>0.6004304498602445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.3426857567443765</td>\n",
       "      <td>0.29482682250720826</td>\n",
       "      <td>0.6004304597005333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Standard Scaler</td>\n",
       "      <td>0.3426857567444026</td>\n",
       "      <td>0.2948268456209845</td>\n",
       "      <td>0.6004304498602445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinMax Scaler</td>\n",
       "      <td>0.3426857567444026</td>\n",
       "      <td>0.2948268456209845</td>\n",
       "      <td>0.6004304498602445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge using Scaler</td>\n",
       "      <td>0.3411910401469027</td>\n",
       "      <td>0.2979710480382105</td>\n",
       "      <td>0.5990903648307659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>0.34333078562346575</td>\n",
       "      <td>0.3372789791018367</td>\n",
       "      <td>0.5990903648307659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Method          R2 Training              R2 Test  \\\n",
       "0   Linear Regression   0.3426857567444026   0.2948268456209844   \n",
       "1               Lasso   0.3426857567443765  0.29482682250720826   \n",
       "2     Standard Scaler   0.3426857567444026   0.2948268456209845   \n",
       "3       MinMax Scaler   0.3426857567444026   0.2948268456209845   \n",
       "4  Ridge using Scaler   0.3411910401469027   0.2979710480382105   \n",
       "5          Polynomial  0.34333078562346575   0.3372789791018367   \n",
       "\n",
       "                 RMSE  \n",
       "0  0.6004304498602445  \n",
       "1  0.6004304597005333  \n",
       "2  0.6004304498602445  \n",
       "3  0.6004304498602445  \n",
       "4  0.5990903648307659  \n",
       "5  0.5990903648307659  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.vstack((method, r2_list_train,r2_list_test, rmse_list)).T\n",
    "results_df = pd.DataFrame(results, columns=['Method', 'R2 Training','R2 Test', 'RMSE'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a457c5",
   "metadata": {},
   "source": [
    "## Anaysis\n",
    "In this table, it can be observed that the performance of this dataset is pretty close (lower values R^2) in the different models, which generally means that this dataset has low correlation and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d76f52",
   "metadata": {},
   "source": [
    "### Part 6: Discussion \n",
    "\n",
    "1. Create a cell (a Markdown cell) in the Jupyter notebook and answer: From managerial perspective, how can one improve the quality of the wine? Explain your answer.\n",
    "\n",
    "\n",
    "- Try with polynomial model\n",
    "- Add fetures\n",
    "- Add data\n",
    "\n",
    "\n",
    "2. Create a cell (a Markdown cell) in the Jupyter notebook and answer: What is overfitting? How linear regression with Ridge solve the problem of overfitting?\n",
    "\n",
    "\n",
    "\n",
    "In linear regression, overfitting can occur when the model is too complex and captures noise in the data as if it were a meaningful pattern. \n",
    "\n",
    "Ridge regression is a technique used to address overfitting in linear regression by adding a regularization term to the cost function.\n",
    "\n",
    "Key points about how Ridge regression addresses overfitting:\n",
    "\n",
    "1.Shrinkage of Coefficients: The regularization term penalizes large coefficients, preventing them from becoming too extreme. This helps to avoid fitting the noise in the training data.\n",
    "\n",
    "2.Reduced Sensitivity to Outliers: Ridge regression is more robust to outliers and noisy data because it reduces the impact of individual data points on the model.\n",
    "\n",
    "3.Better Generalization: By preventing the model from becoming too complex, Ridge regression improves its ability to generalize to new, unseen data.\n",
    "\n",
    "4.Multicollinearity Handling: Ridge regression is particularly useful when there is multicollinearity (high correlation) among the features. It can stabilize and improve the numerical stability of the coefficient estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
